{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m64GooBfkoUC"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "\n",
        "# List of URLs to process\n",
        "URLS = [\n",
        "\n",
        "]\n",
        "\n",
        "# Timeout settings\n",
        "NAVIGATION_TIMEOUT = 180000  # 180 seconds\n",
        "REQUEST_TIMEOUT = 45000  # 45 seconds\n",
        "SELECTOR_TIMEOUT = 45000  # 45 seconds\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "def sanitize_filename(title):\n",
        "    \"\"\"Create a safe filename from article title\"\"\"\n",
        "    keep_chars = (' ', '.', '_', '-')\n",
        "    return \"\".join(c for c in title if c.isalnum() or c in keep_chars).rstrip()\n",
        "\n",
        "async def try_loading_page(page, url):\n",
        "    \"\"\"Multiple strategies to load page content\"\"\"\n",
        "    strategies = [\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.reload(timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"commit\")\n",
        "    ]\n",
        "\n",
        "    for i, strategy in enumerate(strategies, 1):\n",
        "        try:\n",
        "            print(f\"Attempting strategy {i}...\")\n",
        "            await strategy()\n",
        "\n",
        "            wait_strategies = [\n",
        "                lambda: page.wait_for_load_state(\"networkidle\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"article, .article, .articleContent, .content, .story, .main-content, .Normal\",\n",
        "                                            state=\"attached\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"h1, h2, p\", state=\"attached\", timeout=SELECTOR_TIMEOUT)\n",
        "            ]\n",
        "\n",
        "            for j, wait_strategy in enumerate(wait_strategies, 1):\n",
        "                try:\n",
        "                    print(f\"Trying wait strategy {j}...\")\n",
        "                    await wait_strategy()\n",
        "                    print(\"Page loaded successfully\")\n",
        "                    return True\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Strategy {i} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return False\n",
        "\n",
        "async def extract_article_metadata(page):\n",
        "    \"\"\"Ultra-fast metadata extraction with direct DOM access\"\"\"\n",
        "    try:\n",
        "        # Get title and CSM number first (fast operations)\n",
        "        title = (await page.title()).replace(' - The Economic Times', '').strip()\n",
        "        csm_number = page.url.split('/')[-1].split('.')[0]\n",
        "\n",
        "        # 1. FIRST TRY: Directly access the time element's data-dt attribute (fastest)\n",
        "        try:\n",
        "            epoch_ms = await page.evaluate('''() => {\n",
        "                const el = document.querySelector('time.jsdtTime');\n",
        "                return el ? el.getAttribute('data-dt') : null;\n",
        "            }''')\n",
        "            if epoch_ms:\n",
        "                dt = datetime.fromtimestamp(int(epoch_ms)/1000)\n",
        "                return {\n",
        "                    'title': title,\n",
        "                    'csm_number': csm_number,\n",
        "                    'published_date': dt.strftime('%Y%m%d'),\n",
        "                    'display_date': dt.strftime('%d %B %Y')  # Added display format\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 2. SECOND TRY: Direct text extraction from time element (fast)\n",
        "        try:\n",
        "            date_text = await page.evaluate('''() => {\n",
        "                const el = document.querySelector('time.jsdtTime');\n",
        "                return el ? el.textContent : null;\n",
        "            }''')\n",
        "\n",
        "            if date_text and \"Last Updated:\" in date_text:\n",
        "                # Extract just \"Mar 28, 2025\" part\n",
        "                date_part = date_text.split(\"Last Updated:\")[1].split(\",\")[0].strip()\n",
        "                dt = datetime.strptime(date_part, '%b %d %Y')\n",
        "                return {\n",
        "                    'title': title,\n",
        "                    'csm_number': csm_number,\n",
        "                    'published_date': dt.strftime('%Y%m%d'),\n",
        "                    'display_date': dt.strftime('%d %B %Y')  # Added display format\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 3. THIRD TRY: Check for common meta tags (still relatively fast)\n",
        "        try:\n",
        "            meta_date = await page.evaluate('''() => {\n",
        "                const el = document.querySelector('meta[property=\"article:published_time\"]');\n",
        "                return el ? el.content : null;\n",
        "            }''')\n",
        "            if meta_date:\n",
        "                dt = datetime.strptime(meta_date.split('T')[0], '%Y-%m-%d')\n",
        "                return {\n",
        "                    'title': title,\n",
        "                    'csm_number': csm_number,\n",
        "                    'published_date': dt.strftime('%Y%m%d'),\n",
        "                    'display_date': dt.strftime('%d %B %Y')  # Added display format\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Final fallback\n",
        "        current_date = datetime.now()\n",
        "        return {\n",
        "            'title': title,\n",
        "            'csm_number': csm_number,\n",
        "            'published_date': current_date.strftime('%Y%m%d'),\n",
        "            'display_date': current_date.strftime('%d %B %Y')  # Added display format\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Metadata error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(\n",
        "                timeout=NAVIGATION_TIMEOUT,\n",
        "                headless=True,\n",
        "                args=[\n",
        "                    '--disable-gpu',\n",
        "                    '--disable-dev-shm-usage',\n",
        "                    '--disable-setuid-sandbox',\n",
        "                    '--no-sandbox'\n",
        "                ]\n",
        "            )\n",
        "            context = await browser.new_context(\n",
        "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                viewport={'width': 1920, 'height': 1080},\n",
        "                java_script_enabled=True\n",
        "            )\n",
        "            page = await context.new_page()\n",
        "            page.set_default_timeout(NAVIGATION_TIMEOUT)\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=REQUEST_TIMEOUT\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            if not await try_loading_page(page, url):\n",
        "                print(\"All loading strategies failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Extract metadata first (optimized for time.jsdtTime)\n",
        "            metadata = await extract_article_metadata(page)\n",
        "            if not metadata:\n",
        "                print(\"Failed to extract article metadata\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            selectors_to_remove = [\n",
        "                '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                'header', 'footer', 'iframe', 'script', '.comments',\n",
        "                '.related-news', '.recommendations', '.signup-promo',\n",
        "                '.subscribe', '.hidden', '.modal', '.popup', '.leaderboard',\n",
        "                '.ad-container', '.teaser', '.promo', '.newsletter-signup',\n",
        "                '.social-media', '.sharing', '.recommended', '.trending',\n",
        "                '.most-popular', '.also-read', '.more-from-section',\n",
        "                'div[data-ga*=\"Discover\"]', 'div[data-ga*=\"discover\"]',\n",
        "                'div[class*=\"discover\"]', 'div[class*=\"Discover\"]',\n",
        "                'div[data-testid*=\"discover\"]', 'div[id*=\"discover\"]',\n",
        "                'div:has-text(\"Discover the stories\")',\n",
        "                'div:has-text(\"discover the stories\")',\n",
        "                'div:has-text(\"Stay on top\")',\n",
        "                'div:has-text(\"stay on top\")'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors_to_remove:\n",
        "                try:\n",
        "                    await page.evaluate(f\"\"\"selector => {{\n",
        "                        const elements = document.querySelectorAll(selector);\n",
        "                        elements.forEach(el => el.remove());\n",
        "                    }}\"\"\", selector)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Additional text content removal\n",
        "            await page.evaluate(\"\"\"() => {\n",
        "                const unwantedTextPatterns = [\n",
        "                    'Discover the stories of your interest',\n",
        "                    'discover the stories of your interest',\n",
        "                    'Stay on top of technology and startup news',\n",
        "                    'ETPrime stories of the day'\n",
        "                ];\n",
        "\n",
        "                function walkDOM(node) {\n",
        "                    if (node.nodeType === Node.ELEMENT_NODE) {\n",
        "                        if (unwantedTextPatterns.some(pattern =>\n",
        "                            node.textContent.includes(pattern))) {\n",
        "                            if (node.textContent.length < 500) {\n",
        "                                node.remove();\n",
        "                                return;\n",
        "                            }\n",
        "                        }\n",
        "                        Array.from(node.childNodes).forEach(walkDOM);\n",
        "                    }\n",
        "                }\n",
        "                walkDOM(document.body);\n",
        "            }\"\"\")\n",
        "\n",
        "            # Extract article content\n",
        "            article_json = None\n",
        "            extraction_attempts = [\n",
        "                f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        if (!reader) return null;\n",
        "                        return {{\n",
        "                            title: reader.title,\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline,\n",
        "                            excerpt: reader.excerpt\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        console.error('Readability error:', e);\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\",\n",
        "                \"\"\"() => {\n",
        "                    const article = document.querySelector('article, .article, .articleContent') ||\n",
        "                                  document.querySelector('.content, .story, .main-content, .Normal');\n",
        "                    if (!article) return null;\n",
        "                    return {\n",
        "                        title: document.title,\n",
        "                        content: article.innerHTML,\n",
        "                        byline: document.querySelector('.byline, .author, .publish-date')?.textContent || '',\n",
        "                        excerpt: document.querySelector('.excerpt, .summary, .synopsis')?.textContent || ''\n",
        "                    };\n",
        "                }\"\"\"\n",
        "            ]\n",
        "\n",
        "            for attempt, extraction_script in enumerate(extraction_attempts, 1):\n",
        "                try:\n",
        "                    article_json = await page.evaluate(extraction_script)\n",
        "                    if article_json and article_json.get('content'):\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if not article_json or not article_json.get('content'):\n",
        "                print(\"All extraction methods failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Generate PDF with date under heading\n",
        "            try:\n",
        "                await page.set_content(f\"\"\"\n",
        "                    <html>\n",
        "                        <head>\n",
        "                            <meta charset=\"UTF-8\">\n",
        "                            <title>{article_json['title']}</title>\n",
        "                            <style>\n",
        "                                body {{\n",
        "                                    max-width: 800px;\n",
        "                                    margin: 0 auto;\n",
        "                                    padding: 20px;\n",
        "                                    font-family: Arial, sans-serif;\n",
        "                                    line-height: 1.6;\n",
        "                                    color: #333;\n",
        "                                }}\n",
        "                                h1 {{\n",
        "                                    font-size: 24px;\n",
        "                                    margin-bottom: 5px;\n",
        "                                    color: #222;\n",
        "                                }}\n",
        "                                .article-date {{\n",
        "                                    color: #666;\n",
        "                                    margin-bottom: 15px;\n",
        "                                    font-size: 14px;\n",
        "                                }}\n",
        "                                .byline {{\n",
        "                                    color: #666;\n",
        "                                    margin-bottom: 20px;\n",
        "                                    font-style: italic;\n",
        "                                }}\n",
        "                                .excerpt {{\n",
        "                                    font-weight: bold;\n",
        "                                    margin-bottom: 20px;\n",
        "                                    color: #444;\n",
        "                                }}\n",
        "                                img {{\n",
        "                                    max-width: 100%;\n",
        "                                    height: auto;\n",
        "                                    margin: 10px 0;\n",
        "                                }}\n",
        "                                a {{\n",
        "                                    color: #0066cc;\n",
        "                                    text-decoration: none;\n",
        "                                }}\n",
        "                                @media print {{\n",
        "                                    body {{ padding: 0; }}\n",
        "                                }}\n",
        "                            </style>\n",
        "                        </head>\n",
        "                        <body>\n",
        "                            <h1>{article_json['title']}</h1>\n",
        "                            <div class=\"article-date\">{metadata['display_date']}</div>\n",
        "                            {f'<div class=\"byline\">{article_json[\"byline\"]}</div>' if article_json.get(\"byline\") else ''}\n",
        "                            {f'<div class=\"excerpt\">{article_json[\"excerpt\"]}</div>' if article_json.get(\"excerpt\") else ''}\n",
        "                            {article_json['content']}\n",
        "                            <div style=\"margin-top: 30px; font-size: 12px; color: #999;\">\n",
        "                                Source: <a href=\"{url}\">{url}</a>\n",
        "                            </div>\n",
        "                        </body>\n",
        "                    </html>\n",
        "                \"\"\")\n",
        "\n",
        "                await page.pdf(\n",
        "                    path=pdf_path,\n",
        "                    format='A4',\n",
        "                    margin={\n",
        "                        'top': '20mm',\n",
        "                        'right': '20mm',\n",
        "                        'bottom': '20mm',\n",
        "                        'left': '20mm'\n",
        "                    },\n",
        "                    print_background=False,\n",
        "                    scale=0.9\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"PDF generation failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            await browser.close()\n",
        "            return metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "async def process_urls_with_retry():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    batch_dir = os.path.join(OUTPUT_DIR, f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "    os.makedirs(batch_dir, exist_ok=True)\n",
        "\n",
        "    failed = []\n",
        "    success_count = 0\n",
        "\n",
        "    for url in URLS:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing URL: {url}\")\n",
        "\n",
        "        for retry in range(2):  # Max 2 retries\n",
        "            try:\n",
        "                # Create temp path first\n",
        "                temp_pdf = os.path.join(batch_dir, \"temp.pdf\")\n",
        "\n",
        "                # Process article and get metadata\n",
        "                result = await create_clean_article_pdf(url, temp_pdf)\n",
        "                if not result:\n",
        "                    raise Exception(\"PDF creation failed\")\n",
        "\n",
        "                # Generate final filename\n",
        "                safe_title = sanitize_filename(result['title'][:50])\n",
        "                pdf_name = f\"{result['published_date']}_{result['csm_number']}_{safe_title}.pdf\"\n",
        "                final_path = os.path.join(batch_dir, pdf_name)\n",
        "\n",
        "                # Rename temp file to final filename\n",
        "                os.rename(temp_pdf, final_path)\n",
        "\n",
        "                print(f\"Successfully created: {pdf_name}\")\n",
        "                success_count += 1\n",
        "                if success_count == 1:\n",
        "                    display(HTML(f'<a href=\"{final_path}\" download>Download First PDF: {pdf_name}</a>'))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if retry == 1:  # Last attempt failed\n",
        "                    print(f\"Failed to process {url}: {str(e)}\")\n",
        "                    failed.append({'url': url, 'error': str(e)})\n",
        "                else:\n",
        "                    print(f\"Attempt {retry + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(3)\n",
        "                continue\n",
        "\n",
        "    # Save failed URLs if any\n",
        "    if failed:\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerows(failed)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Successfully processed: {success_count}/{len(URLS)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "\n",
        "    if success_count > 0:\n",
        "        display(HTML(f'<a href=\"{batch_dir}\" download>Download All PDFs</a>'))\n",
        "\n",
        "    if failed:\n",
        "        print(\"\\nFailed URLs:\")\n",
        "        for item in failed:\n",
        "            print(f\"- {item['url']}\")\n",
        "            print(f\"  Reason: {item['error']}\")\n",
        "\n",
        "# Step 6: Run the process\n",
        "await process_urls_with_retry()"
      ]
    }
  ]
}