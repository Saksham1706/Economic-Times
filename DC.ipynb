{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMoW41vySF77",
        "outputId": "60733189-4433-4741-8fdd-cecdb6151229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n"
          ]
        }
      ],
      "source": [
        "!pip install playwright requests\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from urllib.parse import urljoin\n",
        "from playwright.sync_api import sync_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML"
      ],
      "metadata": {
        "id": "Yo2psYW0TzIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"/content/articles\"  # Using /content in Colab\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "TARGET_URL = \"http://economictimes.indiatimes.com/tech/newsletters/morning-dispatch/pcis-mdr-plea-indias-ai-startups-accelerated/articleshow/119449885.cms\"\n"
      ],
      "metadata": {
        "id": "wLXMRu9ZUlzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        with sync_playwright() as p:\n",
        "            browser = p.chromium.launch()\n",
        "            page = browser.new_page()\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=10\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                return False\n",
        "\n",
        "            # Navigation\n",
        "            try:\n",
        "                page.goto(url, timeout=60000)\n",
        "                # Wait for content to load\n",
        "                page.wait_for_load_state(\"networkidle\")\n",
        "            except Exception as e:\n",
        "                print(f\"Navigation failed: {str(e)}\")\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            try:\n",
        "                page.evaluate(\"\"\"() => {\n",
        "                    const safeRemove = selector => {\n",
        "                        document.querySelectorAll(selector).forEach(el => el.remove());\n",
        "                    };\n",
        "                    [\n",
        "                        '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                        '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                        'header', 'footer', 'iframe', 'script'\n",
        "                    ].forEach(safeRemove);\n",
        "                }\"\"\")\n",
        "            except Exception as e:\n",
        "                print(f\"Element removal failed: {str(e)}\")\n",
        "\n",
        "            # Extract content\n",
        "            try:\n",
        "                article_json = page.evaluate(f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        return {{\n",
        "                            title: reader.title.replace(' - The Economic Times', ''),\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\")\n",
        "                if not article_json:\n",
        "                    raise Exception(\"Content extraction failed\")\n",
        "            except Exception as e:\n",
        "                print(f\"Content error: {str(e)}\")\n",
        "                return False\n",
        "\n",
        "            # Generate PDF\n",
        "            try:\n",
        "                page.set_content(f\"\"\"\n",
        "                    <html>\n",
        "                        <head>\n",
        "                            <style>\n",
        "                                body {{\n",
        "                                    max-width: 800px;\n",
        "                                    margin: 0 auto;\n",
        "                                    padding: 20px;\n",
        "                                    font-family: Arial;\n",
        "                                    line-height: 1.6;\n",
        "                                }}\n",
        "                                img {{ max-width: 100%; }}\n",
        "                                .byline {{ color: #666; margin-bottom: 20px; }}\n",
        "                            </style>\n",
        "                        </head>\n",
        "                        <body>\n",
        "                            <h1>{article_json['title']}</h1>\n",
        "                            <div class=\"byline\">{article_json['byline']}</div>\n",
        "                            {article_json['content']}\n",
        "                        </body>\n",
        "                    </html>\n",
        "                \"\"\")\n",
        "\n",
        "                # Fixed PDF margin configuration\n",
        "                page.pdf(\n",
        "                    path=pdf_path,\n",
        "                    format='A4',\n",
        "                    margin={\n",
        "                        'top': '15mm',\n",
        "                        'right': '15mm',\n",
        "                        'bottom': '15mm',\n",
        "                        'left': '15mm'\n",
        "                    },\n",
        "                    print_background=False\n",
        "                )\n",
        "                return True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"PDF generation failed: {str(e)}\")\n",
        "                return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "OB1JkbPiUnz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_url():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    url = TARGET_URL\n",
        "    pdf_name = \"article.pdf\"\n",
        "    pdf_path = os.path.join(OUTPUT_DIR, pdf_name)\n",
        "\n",
        "    print(f\"Processing URL: {url}\")\n",
        "\n",
        "    try:\n",
        "        if not create_clean_article_pdf(url, pdf_path):\n",
        "            raise Exception(\"Processing failed\")\n",
        "        print(\"\\nSuccessfully created PDF!\")\n",
        "\n",
        "        # Display download link in Colab\n",
        "        display(HTML(f'<a href=\"{pdf_path}\" download>Download PDF</a>'))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed: {str(e)}\")\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerow({'url': url, 'error': str(e)})"
      ],
      "metadata": {
        "id": "fj_I5xPUUrvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "process_single_url()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ut3ArlaUyzP",
        "outputId": "314ca120-6593-4afb-ce0a-6b654b4788ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing URL: http://economictimes.indiatimes.com/tech/newsletters/morning-dispatch/pcis-mdr-plea-indias-ai-startups-accelerated/articleshow/119449885.cms\n",
            "Processing failed: It looks like you are using Playwright Sync API inside the asyncio loop.\n",
            "Please use the Async API instead.\n",
            "Failed: Processing failed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"  # Using /content in Colab\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "TARGET_URL = \"http://economictimes.indiatimes.com/tech/newsletters/morning-dispatch/pcis-mdr-plea-indias-ai-startups-accelerated/articleshow/119449885.cms\"\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch()\n",
        "            page = await browser.new_page()\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=10\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                return False\n",
        "\n",
        "            # Navigation\n",
        "            try:\n",
        "                await page.goto(url, timeout=60000)\n",
        "                await page.wait_for_load_state(\"networkidle\")\n",
        "            except Exception as e:\n",
        "                print(f\"Navigation failed: {str(e)}\")\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            try:\n",
        "                await page.evaluate(\"\"\"() => {\n",
        "                    const safeRemove = selector => {\n",
        "                        document.querySelectorAll(selector).forEach(el => el.remove());\n",
        "                    };\n",
        "                    [\n",
        "                        '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                        '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                        'header', 'footer', 'iframe', 'script'\n",
        "                    ].forEach(safeRemove);\n",
        "                }\"\"\")\n",
        "            except Exception as e:\n",
        "                print(f\"Element removal failed: {str(e)}\")\n",
        "\n",
        "            # Extract content\n",
        "            try:\n",
        "                article_json = await page.evaluate(f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        return {{\n",
        "                            title: reader.title.replace(' - The Economic Times', ''),\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\")\n",
        "                if not article_json:\n",
        "                    raise Exception(\"Content extraction failed\")\n",
        "            except Exception as e:\n",
        "                print(f\"Content error: {str(e)}\")\n",
        "                return False\n",
        "\n",
        "            # Generate PDF\n",
        "            try:\n",
        "                await page.set_content(f\"\"\"\n",
        "                    <html>\n",
        "                        <head>\n",
        "                            <style>\n",
        "                                body {{\n",
        "                                    max-width: 800px;\n",
        "                                    margin: 0 auto;\n",
        "                                    padding: 20px;\n",
        "                                    font-family: Arial;\n",
        "                                    line-height: 1.6;\n",
        "                                }}\n",
        "                                img {{ max-width: 100%; }}\n",
        "                                .byline {{ color: #666; margin-bottom: 20px; }}\n",
        "                            </style>\n",
        "                        </head>\n",
        "                        <body>\n",
        "                            <h1>{article_json['title']}</h1>\n",
        "                            <div class=\"byline\">{article_json['byline']}</div>\n",
        "                            {article_json['content']}\n",
        "                        </body>\n",
        "                    </html>\n",
        "                \"\"\")\n",
        "\n",
        "                # Fixed PDF margin configuration\n",
        "                await page.pdf(\n",
        "                    path=pdf_path,\n",
        "                    format='A4',\n",
        "                    margin={\n",
        "                        'top': '15mm',\n",
        "                        'right': '15mm',\n",
        "                        'bottom': '15mm',\n",
        "                        'left': '15mm'\n",
        "                    },\n",
        "                    print_background=False\n",
        "                )\n",
        "                return True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"PDF generation failed: {str(e)}\")\n",
        "                return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Step 5: Process the single URL\n",
        "async def process_single_url():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    url = TARGET_URL\n",
        "    pdf_name = \"article.pdf\"\n",
        "    pdf_path = os.path.join(OUTPUT_DIR, pdf_name)\n",
        "\n",
        "    print(f\"Processing URL: {url}\")\n",
        "\n",
        "    try:\n",
        "        if not await create_clean_article_pdf(url, pdf_path):\n",
        "            raise Exception(\"Processing failed\")\n",
        "        print(\"\\nSuccessfully created PDF!\")\n",
        "\n",
        "        # Display download link in Colab\n",
        "        display(HTML(f'<a href=\"{pdf_path}\" download>Download PDF</a>'))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed: {str(e)}\")\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerow({'url': url, 'error': str(e)})\n",
        "\n",
        "# Step 6: Run the process\n",
        "# This is the key change for Colab - we need to run the async function properly\n",
        "def run_async_code():\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "    try:\n",
        "        return loop.run_until_complete(process_single_url())\n",
        "    finally:\n",
        "        loop.close()\n",
        "\n",
        "run_async_code()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "id": "gEbAhCoDU02x",
        "outputId": "85e775fb-f662-4f69-c066-a4cd026b6217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot run the event loop while another loop is running",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0a7b86927938>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m \u001b[0mrun_async_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-0a7b86927938>\u001b[0m in \u001b[0;36mrun_async_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_single_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \"\"\"\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    592\u001b[0m                 'Cannot run the event loop while another loop is running')\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot run the event loop while another loop is running"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries with nest-asyncio for Colab compatibility\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"  # Using /content in Colab\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "TARGET_URL = \"http://economictimes.indiatimes.com/tech/newsletters/morning-dispatch/pcis-mdr-plea-indias-ai-startups-accelerated/articleshow/119449885.cms\"\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch()\n",
        "            page = await browser.new_page()\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=10\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Navigation\n",
        "            try:\n",
        "                await page.goto(url, timeout=60000)\n",
        "                await page.wait_for_load_state(\"networkidle\")\n",
        "            except Exception as e:\n",
        "                print(f\"Navigation failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            try:\n",
        "                await page.evaluate(\"\"\"() => {\n",
        "                    const safeRemove = selector => {\n",
        "                        document.querySelectorAll(selector).forEach(el => el.remove());\n",
        "                    };\n",
        "                    [\n",
        "                        '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                        '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                        'header', 'footer', 'iframe', 'script'\n",
        "                    ].forEach(safeRemove);\n",
        "                }\"\"\")\n",
        "            except Exception as e:\n",
        "                print(f\"Element removal failed: {str(e)}\")\n",
        "\n",
        "            # Extract content\n",
        "            try:\n",
        "                article_json = await page.evaluate(f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        return {{\n",
        "                            title: reader.title.replace(' - The Economic Times', ''),\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\")\n",
        "                if not article_json:\n",
        "                    raise Exception(\"Content extraction failed\")\n",
        "            except Exception as e:\n",
        "                print(f\"Content error: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Generate PDF\n",
        "            try:\n",
        "                await page.set_content(f\"\"\"\n",
        "                    <html>\n",
        "                        <head>\n",
        "                            <style>\n",
        "                                body {{\n",
        "                                    max-width: 800px;\n",
        "                                    margin: 0 auto;\n",
        "                                    padding: 20px;\n",
        "                                    font-family: Arial;\n",
        "                                    line-height: 1.6;\n",
        "                                }}\n",
        "                                img {{ max-width: 100%; }}\n",
        "                                .byline {{ color: #666; margin-bottom: 20px; }}\n",
        "                            </style>\n",
        "                        </head>\n",
        "                        <body>\n",
        "                            <h1>{article_json['title']}</h1>\n",
        "                            <div class=\"byline\">{article_json['byline']}</div>\n",
        "                            {article_json['content']}\n",
        "                        </body>\n",
        "                    </html>\n",
        "                \"\"\")\n",
        "\n",
        "                # Fixed PDF margin configuration\n",
        "                await page.pdf(\n",
        "                    path=pdf_path,\n",
        "                    format='A4',\n",
        "                    margin={\n",
        "                        'top': '15mm',\n",
        "                        'right': '15mm',\n",
        "                        'bottom': '15mm',\n",
        "                        'left': '15mm'\n",
        "                    },\n",
        "                    print_background=False\n",
        "                )\n",
        "                await browser.close()\n",
        "                return True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"PDF generation failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Step 5: Process the single URL\n",
        "async def process_single_url():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    url = TARGET_URL\n",
        "    pdf_name = \"article.pdf\"\n",
        "    pdf_path = os.path.join(OUTPUT_DIR, pdf_name)\n",
        "\n",
        "    print(f\"Processing URL: {url}\")\n",
        "\n",
        "    try:\n",
        "        if not await create_clean_article_pdf(url, pdf_path):\n",
        "            raise Exception(\"Processing failed\")\n",
        "        print(\"\\nSuccessfully created PDF!\")\n",
        "\n",
        "        # Display download link in Colab\n",
        "        display(HTML(f'<a href=\"{pdf_path}\" download>Download PDF</a>'))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed: {str(e)}\")\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerow({'url': url, 'error': str(e)})\n",
        "\n",
        "# Step 6: Run the process in Colab's environment\n",
        "await process_single_url()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNh5oeAsVQeg",
        "outputId": "2f467435-fb61-4627-c401-721aa8e96d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "Processing URL: http://economictimes.indiatimes.com/tech/newsletters/morning-dispatch/pcis-mdr-plea-indias-ai-startups-accelerated/articleshow/119449885.cms\n",
            "Navigation failed: Timeout 30000ms exceeded.\n",
            "Failed: Processing failed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "TARGET_URL = \"http://economictimes.indiatimes.com/tech/newsletters/morning-dispatch/pcis-mdr-plea-indias-ai-startups-accelerated/articleshow/119449885.cms\"\n",
        "\n",
        "# Increased timeouts\n",
        "NAVIGATION_TIMEOUT = 120000  # 120 seconds\n",
        "REQUEST_TIMEOUT = 30000  # 30 seconds\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            # Launch browser with increased timeout\n",
        "            browser = await p.chromium.launch(timeout=NAVIGATION_TIMEOUT)\n",
        "            context = await browser.new_context()\n",
        "            page = await context.new_page()\n",
        "\n",
        "            # Set default timeout for all actions\n",
        "            page.set_default_timeout(NAVIGATION_TIMEOUT)\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=REQUEST_TIMEOUT\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Navigation with multiple fallbacks\n",
        "            try:\n",
        "                print(\"Attempting to load page...\")\n",
        "                await page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\")\n",
        "\n",
        "                # Wait for either networkidle or a specific element\n",
        "                try:\n",
        "                    await page.wait_for_load_state(\"networkidle\", timeout=30000)\n",
        "                except:\n",
        "                    print(\"Falling back to element-based waiting\")\n",
        "                    await page.wait_for_selector(\"body\", state=\"attached\", timeout=30000)\n",
        "\n",
        "                print(\"Page loaded successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"Navigation failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements with retries\n",
        "            selectors_to_remove = [\n",
        "                '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                'header', 'footer', 'iframe', 'script'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors_to_remove:\n",
        "                try:\n",
        "                    await page.evaluate(f\"\"\"selector => {{\n",
        "                        document.querySelectorAll(selector).forEach(el => el.remove());\n",
        "                    }}\"\"\", selector)\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not remove {selector}: {str(e)}\")\n",
        "\n",
        "            # Extract content\n",
        "            try:\n",
        "                print(\"Extracting article content...\")\n",
        "                article_json = await page.evaluate(f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        return {{\n",
        "                            title: reader.title.replace(' - The Economic Times', ''),\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        console.error('Readability error:', e);\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\")\n",
        "\n",
        "                if not article_json:\n",
        "                    raise Exception(\"Content extraction failed\")\n",
        "\n",
        "                print(\"Content extracted successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"Content error: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Generate PDF\n",
        "            try:\n",
        "                print(\"Generating PDF...\")\n",
        "                await page.set_content(f\"\"\"\n",
        "                    <html>\n",
        "                        <head>\n",
        "                            <style>\n",
        "                                body {{\n",
        "                                    max-width: 800px;\n",
        "                                    margin: 0 auto;\n",
        "                                    padding: 20px;\n",
        "                                    font-family: Arial;\n",
        "                                    line-height: 1.6;\n",
        "                                }}\n",
        "                                img {{ max-width: 100%; }}\n",
        "                                .byline {{ color: #666; margin-bottom: 20px; }}\n",
        "                            </style>\n",
        "                        </head>\n",
        "                        <body>\n",
        "                            <h1>{article_json['title']}</h1>\n",
        "                            <div class=\"byline\">{article_json['byline']}</div>\n",
        "                            {article_json['content']}\n",
        "                        </body>\n",
        "                    </html>\n",
        "                \"\"\")\n",
        "\n",
        "                # Generate PDF with retry\n",
        "                try:\n",
        "                    await page.pdf(\n",
        "                        path=pdf_path,\n",
        "                        format='A4',\n",
        "                        margin={\n",
        "                            'top': '15mm',\n",
        "                            'right': '15mm',\n",
        "                            'bottom': '15mm',\n",
        "                            'left': '15mm'\n",
        "                        },\n",
        "                        print_background=False\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"First PDF attempt failed, retrying: {str(e)}\")\n",
        "                    await page.pdf(\n",
        "                        path=pdf_path,\n",
        "                        format='A4',\n",
        "                        margin={\n",
        "                            'top': '15mm',\n",
        "                            'right': '15mm',\n",
        "                            'bottom': '15mm',\n",
        "                            'left': '15mm'\n",
        "                        },\n",
        "                        print_background=False\n",
        "                    )\n",
        "\n",
        "                await browser.close()\n",
        "                return True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"PDF generation failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Step 5: Process the single URL\n",
        "async def process_single_url():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    url = TARGET_URL\n",
        "    pdf_name = \"article.pdf\"\n",
        "    pdf_path = os.path.join(OUTPUT_DIR, pdf_name)\n",
        "\n",
        "    print(f\"Processing URL: {url}\")\n",
        "\n",
        "    try:\n",
        "        if not await create_clean_article_pdf(url, pdf_path):\n",
        "            raise Exception(\"Processing failed\")\n",
        "        print(\"\\nSuccessfully created PDF!\")\n",
        "\n",
        "        # Display download link in Colab\n",
        "        display(HTML(f'<a href=\"{pdf_path}\" download>Download PDF</a>'))\n",
        "    except Exception as e:\n",
        "        print(f\"Failed: {str(e)}\")\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerow({'url': url, 'error': str(e)})\n",
        "\n",
        "# Step 6: Run the process\n",
        "await process_single_url()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "2sv5fWxuVsij",
        "outputId": "3ef30ba1-191d-430e-ad03-2fea5ba86c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "Processing URL: http://economictimes.indiatimes.com/tech/newsletters/morning-dispatch/pcis-mdr-plea-indias-ai-startups-accelerated/articleshow/119449885.cms\n",
            "Attempting to load page...\n",
            "Falling back to element-based waiting\n",
            "Page loaded successfully\n",
            "Extracting article content...\n",
            "Content extracted successfully\n",
            "Generating PDF...\n",
            "\n",
            "Successfully created PDF!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/article.pdf\" download>Download PDF</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "\n",
        "# List of URLs to process\n",
        "URLS = [\n",
        "    \"https://economictimes.indiatimes.com/tech/newsletters/tech-top-5/payments-body-seeks-mdr-return-turkey-opposition-x-ed-out/articleshow/119428423.cms\",\n",
        "    \"https://economictimes.indiatimes.com/tech/technology/payments-body-writes-to-pmo-seeks-return-of-0-3-mdr-on-upi-for-large-merchants-and-rupay-debit-cards/articleshow/119423280.cms\",\n",
        "    \"https://economictimes.indiatimes.com/news/new-updates/cheating-but-still-filing-fake-rape-case-rippling-cofounder-prasanna-sankar-makes-sensational-allegation-against-wife/articleshow/119410129.cms\",\n",
        "    \"https://economictimes.indiatimes.com/tech/newsletters/ettech-unwrapped/top-tech-and-startup-stories-this-week/articleshow/119324661.cms\",\n",
        "    \"https://economictimes.indiatimes.com/markets/stocks/news/sebi-removes-over-70000-misleading-social-media-posts-and-handles-since-october-last-year/articleshow/119325989.cms\"\n",
        "]\n",
        "\n",
        "# Timeout settings\n",
        "NAVIGATION_TIMEOUT = 120000  # 120 seconds\n",
        "REQUEST_TIMEOUT = 30000  # 30 seconds\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "def sanitize_filename(title):\n",
        "    \"\"\"Create a safe filename from article title\"\"\"\n",
        "    keep_chars = (' ', '.', '_', '-')\n",
        "    return \"\".join(c for c in title if c.isalnum() or c in keep_chars).rstrip()\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            # Launch browser with increased timeout and headless mode\n",
        "            browser = await p.chromium.launch(\n",
        "                timeout=NAVIGATION_TIMEOUT,\n",
        "                headless=True,\n",
        "                args=[\n",
        "                    '--disable-gpu',\n",
        "                    '--disable-dev-shm-usage',\n",
        "                    '--disable-setuid-sandbox',\n",
        "                    '--no-sandbox'\n",
        "                ]\n",
        "            )\n",
        "            context = await browser.new_context(\n",
        "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36',\n",
        "                viewport={'width': 1920, 'height': 1080}\n",
        "            )\n",
        "            page = await context.new_page()\n",
        "\n",
        "            # Set default timeout for all actions\n",
        "            page.set_default_timeout(NAVIGATION_TIMEOUT)\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=REQUEST_TIMEOUT\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Navigation with multiple fallbacks\n",
        "            try:\n",
        "                print(f\"\\nLoading: {url}\")\n",
        "                await page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\")\n",
        "\n",
        "                # Wait for either networkidle or a specific element\n",
        "                try:\n",
        "                    await page.wait_for_load_state(\"networkidle\", timeout=30000)\n",
        "                except:\n",
        "                    print(\"Using fallback waiting strategy\")\n",
        "                    await page.wait_for_selector(\"article, .article, .content, .story, .main-content\",\n",
        "                                              state=\"attached\", timeout=30000)\n",
        "\n",
        "                print(\"Page loaded successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"Navigation failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements with retries\n",
        "            selectors_to_remove = [\n",
        "                '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                'header', 'footer', 'iframe', 'script', '.comments',\n",
        "                '.related-news', '.recommendations', '.signup-promo',\n",
        "                '.subscribe', '.hidden', '.modal', '.popup'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors_to_remove:\n",
        "                try:\n",
        "                    await page.evaluate(f\"\"\"selector => {{\n",
        "                        const elements = document.querySelectorAll(selector);\n",
        "                        elements.forEach(el => el.remove());\n",
        "                    }}\"\"\", selector)\n",
        "                except Exception as e:\n",
        "                    pass  # Silently fail if selector doesn't exist\n",
        "\n",
        "            # Extract content\n",
        "            try:\n",
        "                print(\"Extracting article content...\")\n",
        "                article_json = await page.evaluate(f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        if (!reader) {{\n",
        "                            console.error('Readability returned null');\n",
        "                            return null;\n",
        "                        }}\n",
        "                        return {{\n",
        "                            title: reader.title.replace(' - The Economic Times', ''),\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline,\n",
        "                            excerpt: reader.excerpt\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        console.error('Readability error:', e);\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\")\n",
        "\n",
        "                if not article_json or not article_json.get('content'):\n",
        "                    raise Exception(\"Content extraction failed\")\n",
        "\n",
        "                print(f\"Extracted: {article_json['title'][:50]}...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Content error: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Generate PDF\n",
        "            try:\n",
        "                print(\"Generating PDF...\")\n",
        "                await page.set_content(f\"\"\"\n",
        "                    <html>\n",
        "                        <head>\n",
        "                            <meta charset=\"UTF-8\">\n",
        "                            <title>{article_json['title']}</title>\n",
        "                            <style>\n",
        "                                body {{\n",
        "                                    max-width: 800px;\n",
        "                                    margin: 0 auto;\n",
        "                                    padding: 20px;\n",
        "                                    font-family: Arial, sans-serif;\n",
        "                                    line-height: 1.6;\n",
        "                                    color: #333;\n",
        "                                }}\n",
        "                                h1 {{\n",
        "                                    font-size: 24px;\n",
        "                                    margin-bottom: 10px;\n",
        "                                    color: #222;\n",
        "                                }}\n",
        "                                .byline {{\n",
        "                                    color: #666;\n",
        "                                    margin-bottom: 20px;\n",
        "                                    font-style: italic;\n",
        "                                }}\n",
        "                                .excerpt {{\n",
        "                                    font-weight: bold;\n",
        "                                    margin-bottom: 20px;\n",
        "                                    color: #444;\n",
        "                                }}\n",
        "                                img {{\n",
        "                                    max-width: 100%;\n",
        "                                    height: auto;\n",
        "                                    margin: 10px 0;\n",
        "                                }}\n",
        "                                a {{\n",
        "                                    color: #0066cc;\n",
        "                                    text-decoration: none;\n",
        "                                }}\n",
        "                                @media print {{\n",
        "                                    body {{ padding: 0; }}\n",
        "                                }}\n",
        "                            </style>\n",
        "                        </head>\n",
        "                        <body>\n",
        "                            <h1>{article_json['title']}</h1>\n",
        "                            {f'<div class=\"byline\">{article_json[\"byline\"]}</div>' if article_json.get(\"byline\") else ''}\n",
        "                            {f'<div class=\"excerpt\">{article_json[\"excerpt\"]}</div>' if article_json.get(\"excerpt\") else ''}\n",
        "                            {article_json['content']}\n",
        "                            <div style=\"margin-top: 30px; font-size: 12px; color: #999;\">\n",
        "                                Source: <a href=\"{url}\">{url}</a>\n",
        "                            </div>\n",
        "                        </body>\n",
        "                    </html>\n",
        "                \"\"\")\n",
        "\n",
        "                # Generate PDF with retry\n",
        "                max_attempts = 2\n",
        "                for attempt in range(max_attempts):\n",
        "                    try:\n",
        "                        await page.pdf(\n",
        "                            path=pdf_path,\n",
        "                            format='A4',\n",
        "                            margin={\n",
        "                                'top': '20mm',\n",
        "                                'right': '20mm',\n",
        "                                'bottom': '20mm',\n",
        "                                'left': '20mm'\n",
        "                            },\n",
        "                            print_background=False,\n",
        "                            scale=0.9\n",
        "                        )\n",
        "                        break\n",
        "                    except Exception as e:\n",
        "                        if attempt == max_attempts - 1:\n",
        "                            raise\n",
        "                        print(f\"PDF attempt {attempt + 1} failed, retrying...\")\n",
        "                        await asyncio.sleep(2)\n",
        "\n",
        "                await browser.close()\n",
        "                return True\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"PDF generation failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Step 5: Process multiple URLs\n",
        "async def process_urls():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = os.path.join(OUTPUT_DIR, f\"batch_{timestamp}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    failed = []\n",
        "    success_count = 0\n",
        "\n",
        "    for index, url in enumerate(URLS, 1):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing URL {index} of {len(URLS)}\")\n",
        "\n",
        "        try:\n",
        "            # First get the title to use in filename\n",
        "            async with async_playwright() as p:\n",
        "                browser = await p.chromium.launch()\n",
        "                page = await browser.new_page()\n",
        "                await page.goto(url, wait_until=\"domcontentloaded\", timeout=30000)\n",
        "                title = await page.title()\n",
        "                await browser.close()\n",
        "\n",
        "            safe_title = sanitize_filename(title[:50])  # Limit title length for filename\n",
        "            pdf_name = f\"{index}_{safe_title}.pdf\"\n",
        "            pdf_path = os.path.join(output_dir, pdf_name)\n",
        "\n",
        "            if await create_clean_article_pdf(url, pdf_path):\n",
        "                print(f\"Successfully created: {pdf_name}\")\n",
        "                success_count += 1\n",
        "                # Display download link for the first successful PDF\n",
        "                if success_count == 1:\n",
        "                    display(HTML(f'<a href=\"{pdf_path}\" download>Download First PDF: {pdf_name}</a>'))\n",
        "            else:\n",
        "                raise Exception(\"PDF creation failed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process {url}: {str(e)}\")\n",
        "            failed.append({'url': url, 'error': str(e)})\n",
        "            continue\n",
        "\n",
        "        # Small delay between requests\n",
        "        await asyncio.sleep(2)\n",
        "\n",
        "    # Save failed URLs to CSV\n",
        "    if failed:\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerows(failed)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Successfully processed: {success_count}/{len(URLS)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "\n",
        "    if success_count > 0:\n",
        "        display(HTML(f'<a href=\"{output_dir}\" download>Download All PDFs</a>'))\n",
        "\n",
        "# Step 6: Run the process\n",
        "await process_urls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qb10tlK4WS54",
        "outputId": "40973d3d-e86a-410d-98b3-94e9d6a3cf30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "\n",
            "==================================================\n",
            "Processing URL 1 of 5\n",
            "\n",
            "Loading: https://economictimes.indiatimes.com/tech/newsletters/tech-top-5/payments-body-seeks-mdr-return-turkey-opposition-x-ed-out/articleshow/119428423.cms\n",
            "Using fallback waiting strategy\n",
            "Navigation failed: Page.wait_for_selector: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "  - waiting for locator(\"article, .article, .content, .story, .main-content\")\n",
            "\n",
            "Failed to process https://economictimes.indiatimes.com/tech/newsletters/tech-top-5/payments-body-seeks-mdr-return-turkey-opposition-x-ed-out/articleshow/119428423.cms: PDF creation failed\n",
            "\n",
            "==================================================\n",
            "Processing URL 2 of 5\n",
            "\n",
            "Loading: https://economictimes.indiatimes.com/tech/technology/payments-body-writes-to-pmo-seeks-return-of-0-3-mdr-on-upi-for-large-merchants-and-rupay-debit-cards/articleshow/119423280.cms\n",
            "Using fallback waiting strategy\n",
            "Page loaded successfully\n",
            "Extracting article content...\n",
            "Extracted: Payments body writes to PMO, seeks return of MDR o...\n",
            "Generating PDF...\n",
            "Successfully created: 2_Payments body writes to PMO seeks return of MDR o.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250325_125541/2_Payments body writes to PMO seeks return of MDR o.pdf\" download>Download First PDF: 2_Payments body writes to PMO seeks return of MDR o.pdf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Processing URL 3 of 5\n",
            "\n",
            "Loading: https://economictimes.indiatimes.com/news/new-updates/cheating-but-still-filing-fake-rape-case-rippling-cofounder-prasanna-sankar-makes-sensational-allegation-against-wife/articleshow/119410129.cms\n",
            "Using fallback waiting strategy\n",
            "Page loaded successfully\n",
            "Extracting article content...\n",
            "Extracted: 'Cheating, but still filing fake rape case': Rippl...\n",
            "Generating PDF...\n",
            "Successfully created: 3_Cheating but still filing fake rape case Rippl.pdf\n",
            "\n",
            "==================================================\n",
            "Processing URL 4 of 5\n",
            "\n",
            "Loading: https://economictimes.indiatimes.com/tech/newsletters/ettech-unwrapped/top-tech-and-startup-stories-this-week/articleshow/119324661.cms\n",
            "Using fallback waiting strategy\n",
            "Navigation failed: Page.wait_for_selector: Timeout 30000ms exceeded.\n",
            "Call log:\n",
            "  - waiting for locator(\"article, .article, .content, .story, .main-content\")\n",
            "\n",
            "Failed to process https://economictimes.indiatimes.com/tech/newsletters/ettech-unwrapped/top-tech-and-startup-stories-this-week/articleshow/119324661.cms: PDF creation failed\n",
            "\n",
            "==================================================\n",
            "Processing URL 5 of 5\n",
            "\n",
            "Loading: https://economictimes.indiatimes.com/markets/stocks/news/sebi-removes-over-70000-misleading-social-media-posts-and-handles-since-october-last-year/articleshow/119325989.cms\n",
            "Using fallback waiting strategy\n",
            "Page loaded successfully\n",
            "Extracting article content...\n",
            "Extracted: Sebi removes over 70,000 misleading social media p...\n",
            "Generating PDF...\n",
            "Successfully created: 5_Sebi Sebi removes over 70000 misleading social m.pdf\n",
            "\n",
            "Processing complete!\n",
            "Successfully processed: 3/5\n",
            "Failed: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250325_125541\" download>Download All PDFs</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "\n",
        "# List of URLs to process\n",
        "URLS = [\n",
        "    \"https://economictimes.indiatimes.com/tech/newsletters/tech-top-5/payments-body-seeks-mdr-return-turkey-opposition-x-ed-out/articleshow/119428423.cms\",\n",
        "    \"https://economictimes.indiatimes.com/tech/technology/payments-body-writes-to-pmo-seeks-return-of-0-3-mdr-on-upi-for-large-merchants-and-rupay-debit-cards/articleshow/119423280.cms\",\n",
        "    \"https://economictimes.indiatimes.com/news/new-updates/cheating-but-still-filing-fake-rape-case-rippling-cofounder-prasanna-sankar-makes-sensational-allegation-against-wife/articleshow/119410129.cms\",\n",
        "    \"https://economictimes.indiatimes.com/tech/newsletters/ettech-unwrapped/top-tech-and-startup-stories-this-week/articleshow/119324661.cms\",\n",
        "    \"https://economictimes.indiatimes.com/markets/stocks/news/sebi-removes-over-70000-misleading-social-media-posts-and-handles-since-october-last-year/articleshow/119325989.cms\"\n",
        "]\n",
        "\n",
        "# Timeout settings (increased from previous version)\n",
        "NAVIGATION_TIMEOUT = 180000  # 180 seconds\n",
        "REQUEST_TIMEOUT = 45000  # 45 seconds\n",
        "SELECTOR_TIMEOUT = 45000  # 45 seconds\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "def sanitize_filename(title):\n",
        "    \"\"\"Create a safe filename from article title\"\"\"\n",
        "    keep_chars = (' ', '.', '_', '-')\n",
        "    return \"\".join(c for c in title if c.isalnum() or c in keep_chars).rstrip()\n",
        "\n",
        "async def try_loading_page(page, url):\n",
        "    \"\"\"Multiple strategies to load page content\"\"\"\n",
        "    strategies = [\n",
        "        # Strategy 1: Standard navigation\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        # Strategy 2: Reload if needed\n",
        "        lambda: page.reload(timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        # Strategy 3: Bypass potential blockers\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"commit\")\n",
        "    ]\n",
        "\n",
        "    for i, strategy in enumerate(strategies, 1):\n",
        "        try:\n",
        "            print(f\"Attempting strategy {i}...\")\n",
        "            await strategy()\n",
        "\n",
        "            # Additional waiting strategies\n",
        "            wait_strategies = [\n",
        "                lambda: page.wait_for_load_state(\"networkidle\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"article, .article, .articleContent, .content, .story, .main-content, .Normal\",\n",
        "                                            state=\"attached\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"h1, h2, p\", state=\"attached\", timeout=SELECTOR_TIMEOUT)\n",
        "            ]\n",
        "\n",
        "            for j, wait_strategy in enumerate(wait_strategies, 1):\n",
        "                try:\n",
        "                    print(f\"Trying wait strategy {j}...\")\n",
        "                    await wait_strategy()\n",
        "                    print(\"Page loaded successfully\")\n",
        "                    return True\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # If we get here, none of the wait strategies worked\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Strategy {i} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return False\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            # Launch browser with increased timeout and headless mode\n",
        "            browser = await p.chromium.launch(\n",
        "                timeout=NAVIGATION_TIMEOUT,\n",
        "                headless=True,\n",
        "                args=[\n",
        "                    '--disable-gpu',\n",
        "                    '--disable-dev-shm-usage',\n",
        "                    '--disable-setuid-sandbox',\n",
        "                    '--no-sandbox'\n",
        "                ]\n",
        "            )\n",
        "            context = await browser.new_context(\n",
        "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                viewport={'width': 1920, 'height': 1080},\n",
        "                java_script_enabled=True\n",
        "            )\n",
        "            page = await context.new_page()\n",
        "\n",
        "            # Set default timeout for all actions\n",
        "            page.set_default_timeout(NAVIGATION_TIMEOUT)\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=REQUEST_TIMEOUT\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Navigation with multiple fallbacks\n",
        "            if not await try_loading_page(page, url):\n",
        "                print(\"All loading strategies failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements with retries\n",
        "            selectors_to_remove = [\n",
        "                '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                'header', 'footer', 'iframe', 'script', '.comments',\n",
        "                '.related-news', '.recommendations', '.signup-promo',\n",
        "                '.subscribe', '.hidden', '.modal', '.popup', '.leaderboard',\n",
        "                '.ad-container', '.teaser', '.promo', '.newsletter-signup',\n",
        "                '.social-media', '.sharing', '.recommended', '.trending',\n",
        "                '.most-popular', '.also-read', '.more-from-section'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors_to_remove:\n",
        "                try:\n",
        "                    await page.evaluate(f\"\"\"selector => {{\n",
        "                        const elements = document.querySelectorAll(selector);\n",
        "                        elements.forEach(el => el.remove());\n",
        "                    }}\"\"\", selector)\n",
        "                except:\n",
        "                    pass  # Silently fail if selector doesn't exist\n",
        "\n",
        "            # Extract content with multiple fallbacks\n",
        "            article_json = None\n",
        "            extraction_attempts = [\n",
        "                # Attempt 1: Standard Readability.js\n",
        "                f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        if (!reader) return null;\n",
        "                        return {{\n",
        "                            title: reader.title.replace(' - The Economic Times', ''),\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline,\n",
        "                            excerpt: reader.excerpt\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        console.error('Readability error:', e);\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\",\n",
        "\n",
        "                # Attempt 2: Fallback to article content extraction\n",
        "                \"\"\"() => {\n",
        "                    const article = document.querySelector('article, .article, .articleContent') ||\n",
        "                                  document.querySelector('.content, .story, .main-content, .Normal');\n",
        "                    if (!article) return null;\n",
        "\n",
        "                    return {\n",
        "                        title: document.title.replace(' - The Economic Times', ''),\n",
        "                        content: article.innerHTML,\n",
        "                        byline: document.querySelector('.byline, .author, .publish-date')?.textContent || '',\n",
        "                        excerpt: document.querySelector('.excerpt, .summary, .synopsis')?.textContent || ''\n",
        "                    };\n",
        "                }\"\"\"\n",
        "            ]\n",
        "\n",
        "            for attempt, extraction_script in enumerate(extraction_attempts, 1):\n",
        "                try:\n",
        "                    print(f\"Attempting extraction method {attempt}...\")\n",
        "                    article_json = await page.evaluate(extraction_script)\n",
        "                    if article_json and article_json.get('content'):\n",
        "                        print(f\"Extracted: {article_json['title'][:50]}...\")\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    print(f\"Extraction attempt {attempt} failed: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if not article_json or not article_json.get('content'):\n",
        "                print(\"All extraction methods failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Generate PDF with multiple attempts\n",
        "            max_pdf_attempts = 3\n",
        "            for attempt in range(max_pdf_attempts):\n",
        "                try:\n",
        "                    print(f\"PDF generation attempt {attempt + 1}...\")\n",
        "                    await page.set_content(f\"\"\"\n",
        "                        <html>\n",
        "                            <head>\n",
        "                                <meta charset=\"UTF-8\">\n",
        "                                <title>{article_json['title']}</title>\n",
        "                                <style>\n",
        "                                    body {{\n",
        "                                        max-width: 800px;\n",
        "                                        margin: 0 auto;\n",
        "                                        padding: 20px;\n",
        "                                        font-family: Arial, sans-serif;\n",
        "                                        line-height: 1.6;\n",
        "                                        color: #333;\n",
        "                                    }}\n",
        "                                    h1 {{\n",
        "                                        font-size: 24px;\n",
        "                                        margin-bottom: 10px;\n",
        "                                        color: #222;\n",
        "                                    }}\n",
        "                                    .byline {{\n",
        "                                        color: #666;\n",
        "                                        margin-bottom: 20px;\n",
        "                                        font-style: italic;\n",
        "                                    }}\n",
        "                                    .excerpt {{\n",
        "                                        font-weight: bold;\n",
        "                                        margin-bottom: 20px;\n",
        "                                        color: #444;\n",
        "                                    }}\n",
        "                                    img {{\n",
        "                                        max-width: 100%;\n",
        "                                        height: auto;\n",
        "                                        margin: 10px 0;\n",
        "                                    }}\n",
        "                                    a {{\n",
        "                                        color: #0066cc;\n",
        "                                        text-decoration: none;\n",
        "                                    }}\n",
        "                                    @media print {{\n",
        "                                        body {{ padding: 0; }}\n",
        "                                    }}\n",
        "                                </style>\n",
        "                            </head>\n",
        "                            <body>\n",
        "                                <h1>{article_json['title']}</h1>\n",
        "                                {f'<div class=\"byline\">{article_json[\"byline\"]}</div>' if article_json.get(\"byline\") else ''}\n",
        "                                {f'<div class=\"excerpt\">{article_json[\"excerpt\"]}</div>' if article_json.get(\"excerpt\") else ''}\n",
        "                                {article_json['content']}\n",
        "                                <div style=\"margin-top: 30px; font-size: 12px; color: #999;\">\n",
        "                                    Source: <a href=\"{url}\">{url}</a>\n",
        "                                </div>\n",
        "                            </body>\n",
        "                        </html>\n",
        "                    \"\"\")\n",
        "\n",
        "                    await page.pdf(\n",
        "                        path=pdf_path,\n",
        "                        format='A4',\n",
        "                        margin={\n",
        "                            'top': '20mm',\n",
        "                            'right': '20mm',\n",
        "                            'bottom': '20mm',\n",
        "                            'left': '20mm'\n",
        "                        },\n",
        "                        print_background=False,\n",
        "                        scale=0.9\n",
        "                    )\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    if attempt == max_pdf_attempts - 1:\n",
        "                        print(f\"PDF generation failed after {max_pdf_attempts} attempts: {str(e)}\")\n",
        "                        await browser.close()\n",
        "                        return False\n",
        "                    print(f\"PDF attempt {attempt + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(2)  # Small delay before retry\n",
        "\n",
        "            await browser.close()\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Step 5: Process multiple URLs with retries\n",
        "async def process_urls_with_retry():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = os.path.join(OUTPUT_DIR, f\"batch_{timestamp}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    failed = []\n",
        "    success_count = 0\n",
        "\n",
        "    for index, url in enumerate(URLS, 1):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing URL {index} of {len(URLS)}\")\n",
        "        print(url)\n",
        "\n",
        "        max_retries = 2\n",
        "        pdf_path = None\n",
        "\n",
        "        for retry in range(max_retries):\n",
        "            try:\n",
        "                # First get the title to use in filename\n",
        "                async with async_playwright() as p:\n",
        "                    browser = await p.chromium.launch()\n",
        "                    page = await browser.new_page()\n",
        "                    try:\n",
        "                        await page.goto(url, wait_until=\"domcontentloaded\", timeout=30000)\n",
        "                        title = await page.title()\n",
        "                    except:\n",
        "                        title = f\"Article_{index}\"\n",
        "                    await browser.close()\n",
        "\n",
        "                safe_title = sanitize_filename(title[:50])  # Limit title length for filename\n",
        "                pdf_name = f\"{index}_{safe_title}.pdf\"\n",
        "                pdf_path = os.path.join(output_dir, pdf_name)\n",
        "\n",
        "                if await create_clean_article_pdf(url, pdf_path):\n",
        "                    print(f\"Successfully created: {pdf_name}\")\n",
        "                    success_count += 1\n",
        "                    # Display download link for the first successful PDF\n",
        "                    if success_count == 1:\n",
        "                        display(HTML(f'<a href=\"{pdf_path}\" download>Download First PDF: {pdf_name}</a>'))\n",
        "                    break  # Success, no need to retry\n",
        "                else:\n",
        "                    if retry == max_retries - 1:\n",
        "                        raise Exception(\"PDF creation failed after retries\")\n",
        "                    print(f\"Attempt {retry + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(5)  # Wait before retry\n",
        "\n",
        "            except Exception as e:\n",
        "                if retry == max_retries - 1:\n",
        "                    print(f\"Failed to process {url}: {str(e)}\")\n",
        "                    failed.append({'url': url, 'error': str(e)})\n",
        "                continue\n",
        "\n",
        "        # Small delay between requests\n",
        "        await asyncio.sleep(3)\n",
        "\n",
        "    # Save failed URLs to CSV\n",
        "    if failed:\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerows(failed)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Successfully processed: {success_count}/{len(URLS)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "\n",
        "    if success_count > 0:\n",
        "        display(HTML(f'<a href=\"{output_dir}\" download>Download All PDFs</a>'))\n",
        "\n",
        "    # Print failed URLs for reference\n",
        "    if failed:\n",
        "        print(\"\\nFailed URLs:\")\n",
        "        for item in failed:\n",
        "            print(f\"- {item['url']}\")\n",
        "            print(f\"  Reason: {item['error']}\")\n",
        "\n",
        "# Step 6: Run the process\n",
        "await process_urls_with_retry()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "au1OlZqaaS36",
        "outputId": "c6f62e00-87da-44d8-e513-d3d3b5af79b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "\n",
            "==================================================\n",
            "Processing URL 1 of 5\n",
            "https://economictimes.indiatimes.com/tech/newsletters/tech-top-5/payments-body-seeks-mdr-return-turkey-opposition-x-ed-out/articleshow/119428423.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Trying wait strategy 2...\n",
            "Trying wait strategy 3...\n",
            "Page loaded successfully\n",
            "Attempting extraction method 1...\n",
            "Extracted: Payments body seeks MDR return; Turkey opposition ...\n",
            "PDF generation attempt 1...\n",
            "Successfully created: 1_Payments body seeks MDR return Turkey opposition.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250325_130628/1_Payments body seeks MDR return Turkey opposition.pdf\" download>Download First PDF: 1_Payments body seeks MDR return Turkey opposition.pdf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Processing URL 2 of 5\n",
            "https://economictimes.indiatimes.com/tech/technology/payments-body-writes-to-pmo-seeks-return-of-0-3-mdr-on-upi-for-large-merchants-and-rupay-debit-cards/articleshow/119423280.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Trying wait strategy 2...\n",
            "Page loaded successfully\n",
            "Attempting extraction method 1...\n",
            "Extracted: Payments body writes to PMO, seeks return of MDR o...\n",
            "PDF generation attempt 1...\n",
            "Successfully created: 2_Payments body writes to PMO seeks return of MDR o.pdf\n",
            "\n",
            "==================================================\n",
            "Processing URL 3 of 5\n",
            "https://economictimes.indiatimes.com/news/new-updates/cheating-but-still-filing-fake-rape-case-rippling-cofounder-prasanna-sankar-makes-sensational-allegation-against-wife/articleshow/119410129.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Trying wait strategy 2...\n",
            "Page loaded successfully\n",
            "Attempting extraction method 1...\n",
            "Extracted: 'Cheating, but still filing fake rape case': Rippl...\n",
            "PDF generation attempt 1...\n",
            "Successfully created: 3_Cheating but still filing fake rape case Rippl.pdf\n",
            "\n",
            "==================================================\n",
            "Processing URL 4 of 5\n",
            "https://economictimes.indiatimes.com/tech/newsletters/ettech-unwrapped/top-tech-and-startup-stories-this-week/articleshow/119324661.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Page loaded successfully\n",
            "Attempting extraction method 1...\n",
            "Extracted: Top tech and startup stories this week...\n",
            "PDF generation attempt 1...\n",
            "Successfully created: 4_Top tech and startup stories this week.pdf\n",
            "\n",
            "==================================================\n",
            "Processing URL 5 of 5\n",
            "https://economictimes.indiatimes.com/markets/stocks/news/sebi-removes-over-70000-misleading-social-media-posts-and-handles-since-october-last-year/articleshow/119325989.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Page loaded successfully\n",
            "Attempting extraction method 1...\n",
            "Extracted: Sebi removes over 70,000 misleading social media p...\n",
            "PDF generation attempt 1...\n",
            "Successfully created: 5_Sebi Sebi removes over 70000 misleading social m.pdf\n",
            "\n",
            "Processing complete!\n",
            "Successfully processed: 5/5\n",
            "Failed: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250325_130628\" download>Download All PDFs</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "\n",
        "# List of URLs to process - only your specified URL\n",
        "URLS = [\n",
        "    \"https://economictimes.indiatimes.com/tech/technology/payu-acquires-payment-solutions-company-mindgate-for-200-250-million/articleshow/119254923.cms\"\n",
        "]\n",
        "\n",
        "# Timeout settings (increased from previous version)\n",
        "NAVIGATION_TIMEOUT = 180000  # 180 seconds\n",
        "REQUEST_TIMEOUT = 45000  # 45 seconds\n",
        "SELECTOR_TIMEOUT = 45000  # 45 seconds\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "def sanitize_filename(title):\n",
        "    \"\"\"Create a safe filename from article title\"\"\"\n",
        "    keep_chars = (' ', '.', '_', '-')\n",
        "    return \"\".join(c for c in title if c.isalnum() or c in keep_chars).rstrip()\n",
        "\n",
        "async def try_loading_page(page, url):\n",
        "    \"\"\"Multiple strategies to load page content\"\"\"\n",
        "    strategies = [\n",
        "        # Strategy 1: Standard navigation\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        # Strategy 2: Reload if needed\n",
        "        lambda: page.reload(timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        # Strategy 3: Bypass potential blockers\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"commit\")\n",
        "    ]\n",
        "\n",
        "    for i, strategy in enumerate(strategies, 1):\n",
        "        try:\n",
        "            print(f\"Attempting strategy {i}...\")\n",
        "            await strategy()\n",
        "\n",
        "            # Additional waiting strategies\n",
        "            wait_strategies = [\n",
        "                lambda: page.wait_for_load_state(\"networkidle\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"article, .article, .articleContent, .content, .story, .main-content, .Normal\",\n",
        "                                            state=\"attached\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"h1, h2, p\", state=\"attached\", timeout=SELECTOR_TIMEOUT)\n",
        "            ]\n",
        "\n",
        "            for j, wait_strategy in enumerate(wait_strategies, 1):\n",
        "                try:\n",
        "                    print(f\"Trying wait strategy {j}...\")\n",
        "                    await wait_strategy()\n",
        "                    print(\"Page loaded successfully\")\n",
        "                    return True\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # If we get here, none of the wait strategies worked\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Strategy {i} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return False\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            # Launch browser with increased timeout and headless mode\n",
        "            browser = await p.chromium.launch(\n",
        "                timeout=NAVIGATION_TIMEOUT,\n",
        "                headless=True,\n",
        "                args=[\n",
        "                    '--disable-gpu',\n",
        "                    '--disable-dev-shm-usage',\n",
        "                    '--disable-setuid-sandbox',\n",
        "                    '--no-sandbox'\n",
        "                ]\n",
        "            )\n",
        "            context = await browser.new_context(\n",
        "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                viewport={'width': 1920, 'height': 1080},\n",
        "                java_script_enabled=True\n",
        "            )\n",
        "            page = await context.new_page()\n",
        "\n",
        "            # Set default timeout for all actions\n",
        "            page.set_default_timeout(NAVIGATION_TIMEOUT)\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=REQUEST_TIMEOUT\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Navigation with multiple fallbacks\n",
        "            if not await try_loading_page(page, url):\n",
        "                print(\"All loading strategies failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements with retries\n",
        "            selectors_to_remove = [\n",
        "                '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                'header', 'footer', 'iframe', 'script', '.comments',\n",
        "                '.related-news', '.recommendations', '.signup-promo',\n",
        "                '.subscribe', '.hidden', '.modal', '.popup', '.leaderboard',\n",
        "                '.ad-container', '.teaser', '.promo', '.newsletter-signup',\n",
        "                '.social-media', '.sharing', '.recommended', '.trending',\n",
        "                '.most-popular', '.also-read', '.more-from-section'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors_to_remove:\n",
        "                try:\n",
        "                    await page.evaluate(f\"\"\"selector => {{\n",
        "                        const elements = document.querySelectorAll(selector);\n",
        "                        elements.forEach(el => el.remove());\n",
        "                    }}\"\"\", selector)\n",
        "                except:\n",
        "                    pass  # Silently fail if selector doesn't exist\n",
        "\n",
        "            # Extract content with multiple fallbacks\n",
        "            article_json = None\n",
        "            extraction_attempts = [\n",
        "                # Attempt 1: Standard Readability.js\n",
        "                f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        if (!reader) return null;\n",
        "                        return {{\n",
        "                            title: reader.title.replace(' - The Economic Times', ''),\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline,\n",
        "                            excerpt: reader.excerpt\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        console.error('Readability error:', e);\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\",\n",
        "\n",
        "                # Attempt 2: Fallback to article content extraction\n",
        "                \"\"\"() => {\n",
        "                    const article = document.querySelector('article, .article, .articleContent') ||\n",
        "                                  document.querySelector('.content, .story, .main-content, .Normal');\n",
        "                    if (!article) return null;\n",
        "\n",
        "                    return {\n",
        "                        title: document.title.replace(' - The Economic Times', ''),\n",
        "                        content: article.innerHTML,\n",
        "                        byline: document.querySelector('.byline, .author, .publish-date')?.textContent || '',\n",
        "                        excerpt: document.querySelector('.excerpt, .summary, .synopsis')?.textContent || ''\n",
        "                    };\n",
        "                }\"\"\"\n",
        "            ]\n",
        "\n",
        "            for attempt, extraction_script in enumerate(extraction_attempts, 1):\n",
        "                try:\n",
        "                    print(f\"Attempting extraction method {attempt}...\")\n",
        "                    article_json = await page.evaluate(extraction_script)\n",
        "                    if article_json and article_json.get('content'):\n",
        "                        print(f\"Extracted: {article_json['title'][:50]}...\")\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    print(f\"Extraction attempt {attempt} failed: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if not article_json or not article_json.get('content'):\n",
        "                print(\"All extraction methods failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Generate PDF with multiple attempts\n",
        "            max_pdf_attempts = 3\n",
        "            for attempt in range(max_pdf_attempts):\n",
        "                try:\n",
        "                    print(f\"PDF generation attempt {attempt + 1}...\")\n",
        "                    await page.set_content(f\"\"\"\n",
        "                        <html>\n",
        "                            <head>\n",
        "                                <meta charset=\"UTF-8\">\n",
        "                                <title>{article_json['title']}</title>\n",
        "                                <style>\n",
        "                                    body {{\n",
        "                                        max-width: 800px;\n",
        "                                        margin: 0 auto;\n",
        "                                        padding: 20px;\n",
        "                                        font-family: Arial, sans-serif;\n",
        "                                        line-height: 1.6;\n",
        "                                        color: #333;\n",
        "                                    }}\n",
        "                                    h1 {{\n",
        "                                        font-size: 24px;\n",
        "                                        margin-bottom: 10px;\n",
        "                                        color: #222;\n",
        "                                    }}\n",
        "                                    .byline {{\n",
        "                                        color: #666;\n",
        "                                        margin-bottom: 20px;\n",
        "                                        font-style: italic;\n",
        "                                    }}\n",
        "                                    .excerpt {{\n",
        "                                        font-weight: bold;\n",
        "                                        margin-bottom: 20px;\n",
        "                                        color: #444;\n",
        "                                    }}\n",
        "                                    img {{\n",
        "                                        max-width: 100%;\n",
        "                                        height: auto;\n",
        "                                        margin: 10px 0;\n",
        "                                    }}\n",
        "                                    a {{\n",
        "                                        color: #0066cc;\n",
        "                                        text-decoration: none;\n",
        "                                    }}\n",
        "                                    @media print {{\n",
        "                                        body {{ padding: 0; }}\n",
        "                                    }}\n",
        "                                </style>\n",
        "                            </head>\n",
        "                            <body>\n",
        "                                <h1>{article_json['title']}</h1>\n",
        "                                {f'<div class=\"byline\">{article_json[\"byline\"]}</div>' if article_json.get(\"byline\") else ''}\n",
        "                                {f'<div class=\"excerpt\">{article_json[\"excerpt\"]}</div>' if article_json.get(\"excerpt\") else ''}\n",
        "                                {article_json['content']}\n",
        "                                <div style=\"margin-top: 30px; font-size: 12px; color: #999;\">\n",
        "                                    Source: <a href=\"{url}\">{url}</a>\n",
        "                                </div>\n",
        "                            </body>\n",
        "                        </html>\n",
        "                    \"\"\")\n",
        "\n",
        "                    await page.pdf(\n",
        "                        path=pdf_path,\n",
        "                        format='A4',\n",
        "                        margin={\n",
        "                            'top': '20mm',\n",
        "                            'right': '20mm',\n",
        "                            'bottom': '20mm',\n",
        "                            'left': '20mm'\n",
        "                        },\n",
        "                        print_background=False,\n",
        "                        scale=0.9\n",
        "                    )\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    if attempt == max_pdf_attempts - 1:\n",
        "                        print(f\"PDF generation failed after {max_pdf_attempts} attempts: {str(e)}\")\n",
        "                        await browser.close()\n",
        "                        return False\n",
        "                    print(f\"PDF attempt {attempt + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(2)  # Small delay before retry\n",
        "\n",
        "            await browser.close()\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Step 5: Process multiple URLs with retries\n",
        "async def process_urls_with_retry():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = os.path.join(OUTPUT_DIR, f\"batch_{timestamp}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    failed = []\n",
        "    success_count = 0\n",
        "\n",
        "    for index, url in enumerate(URLS, 1):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing URL {index} of {len(URLS)}\")\n",
        "        print(url)\n",
        "\n",
        "        max_retries = 2\n",
        "        pdf_path = None\n",
        "\n",
        "        for retry in range(max_retries):\n",
        "            try:\n",
        "                # First get the title to use in filename\n",
        "                async with async_playwright() as p:\n",
        "                    browser = await p.chromium.launch()\n",
        "                    page = await browser.new_page()\n",
        "                    try:\n",
        "                        await page.goto(url, wait_until=\"domcontentloaded\", timeout=30000)\n",
        "                        title = await page.title()\n",
        "                    except:\n",
        "                        title = f\"Article_{index}\"\n",
        "                    await browser.close()\n",
        "\n",
        "                safe_title = sanitize_filename(title[:50])  # Limit title length for filename\n",
        "                pdf_name = f\"{index}_{safe_title}.pdf\"\n",
        "                pdf_path = os.path.join(output_dir, pdf_name)\n",
        "\n",
        "                if await create_clean_article_pdf(url, pdf_path):\n",
        "                    print(f\"Successfully created: {pdf_name}\")\n",
        "                    success_count += 1\n",
        "                    # Display download link for the first successful PDF\n",
        "                    if success_count == 1:\n",
        "                        display(HTML(f'<a href=\"{pdf_path}\" download>Download First PDF: {pdf_name}</a>'))\n",
        "                    break  # Success, no need to retry\n",
        "                else:\n",
        "                    if retry == max_retries - 1:\n",
        "                        raise Exception(\"PDF creation failed after retries\")\n",
        "                    print(f\"Attempt {retry + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(5)  # Wait before retry\n",
        "\n",
        "            except Exception as e:\n",
        "                if retry == max_retries - 1:\n",
        "                    print(f\"Failed to process {url}: {str(e)}\")\n",
        "                    failed.append({'url': url, 'error': str(e)})\n",
        "                continue\n",
        "\n",
        "        # Small delay between requests\n",
        "        await asyncio.sleep(3)\n",
        "\n",
        "    # Save failed URLs to CSV\n",
        "    if failed:\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerows(failed)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Successfully processed: {success_count}/{len(URLS)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "\n",
        "    if success_count > 0:\n",
        "        display(HTML(f'<a href=\"{output_dir}\" download>Download All PDFs</a>'))\n",
        "\n",
        "    # Print failed URLs for reference\n",
        "    if failed:\n",
        "        print(\"\\nFailed URLs:\")\n",
        "        for item in failed:\n",
        "            print(f\"- {item['url']}\")\n",
        "            print(f\"  Reason: {item['error']}\")\n",
        "\n",
        "# Step 6: Run the process\n",
        "await process_urls_with_retry()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "NTEk-F1td6o8",
        "outputId": "e178917c-7101-493e-f301-f0e3468b7488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "\n",
            "==================================================\n",
            "Processing URL 1 of 1\n",
            "https://economictimes.indiatimes.com/tech/technology/payu-acquires-payment-solutions-company-mindgate-for-200-250-million/articleshow/119254923.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Trying wait strategy 2...\n",
            "Page loaded successfully\n",
            "Attempting extraction method 1...\n",
            "Extracted: PayU picks up 43% stake in payments solutions firm...\n",
            "PDF generation attempt 1...\n",
            "Successfully created: 1_PayU picks up 43 stake in payments solutions firm.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250325_132605/1_PayU picks up 43 stake in payments solutions firm.pdf\" download>Download First PDF: 1_PayU picks up 43 stake in payments solutions firm.pdf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing complete!\n",
            "Successfully processed: 1/1\n",
            "Failed: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250325_132605\" download>Download All PDFs</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "\n",
        "# List of URLs to process - only your specified URL\n",
        "URLS = [\n",
        "    \"https://economictimes.indiatimes.com/news/economy/policy/nclat-reduces-googles-936-crore-penalty-to-217-crore-over-competition-law-violation/articleshow/119671499.cms\"\n",
        "]\n",
        "\n",
        "# Timeout settings\n",
        "NAVIGATION_TIMEOUT = 180000  # 180 seconds\n",
        "REQUEST_TIMEOUT = 45000  # 45 seconds\n",
        "SELECTOR_TIMEOUT = 45000  # 45 seconds\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "def sanitize_filename(title):\n",
        "    \"\"\"Create a safe filename from article title\"\"\"\n",
        "    keep_chars = (' ', '.', '_', '-')\n",
        "    return \"\".join(c for c in title if c.isalnum() or c in keep_chars).rstrip()\n",
        "\n",
        "async def try_loading_page(page, url):\n",
        "    \"\"\"Multiple strategies to load page content\"\"\"\n",
        "    strategies = [\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.reload(timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"commit\")\n",
        "    ]\n",
        "\n",
        "    for i, strategy in enumerate(strategies, 1):\n",
        "        try:\n",
        "            print(f\"Attempting strategy {i}...\")\n",
        "            await strategy()\n",
        "\n",
        "            wait_strategies = [\n",
        "                lambda: page.wait_for_load_state(\"networkidle\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"article, .article, .articleContent, .content, .story, .main-content, .Normal\",\n",
        "                                            state=\"attached\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"h1, h2, p\", state=\"attached\", timeout=SELECTOR_TIMEOUT)\n",
        "            ]\n",
        "\n",
        "            for j, wait_strategy in enumerate(wait_strategies, 1):\n",
        "                try:\n",
        "                    print(f\"Trying wait strategy {j}...\")\n",
        "                    await wait_strategy()\n",
        "                    print(\"Page loaded successfully\")\n",
        "                    return True\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Strategy {i} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return False\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(\n",
        "                timeout=NAVIGATION_TIMEOUT,\n",
        "                headless=True,\n",
        "                args=[\n",
        "                    '--disable-gpu',\n",
        "                    '--disable-dev-shm-usage',\n",
        "                    '--disable-setuid-sandbox',\n",
        "                    '--no-sandbox'\n",
        "                ]\n",
        "            )\n",
        "            context = await browser.new_context(\n",
        "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                viewport={'width': 1920, 'height': 1080},\n",
        "                java_script_enabled=True\n",
        "            )\n",
        "            page = await context.new_page()\n",
        "            page.set_default_timeout(NAVIGATION_TIMEOUT)\n",
        "\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=REQUEST_TIMEOUT\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            if not await try_loading_page(page, url):\n",
        "                print(\"All loading strategies failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements with specific focus on \"Discover the stories\" sections\n",
        "            selectors_to_remove = [\n",
        "                '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                'header', 'footer', 'iframe', 'script', '.comments',\n",
        "                '.related-news', '.recommendations', '.signup-promo',\n",
        "                '.subscribe', '.hidden', '.modal', '.popup', '.leaderboard',\n",
        "                '.ad-container', '.teaser', '.promo', '.newsletter-signup',\n",
        "                '.social-media', '.sharing', '.recommended', '.trending',\n",
        "                '.most-popular', '.also-read', '.more-from-section',\n",
        "                # Specific selectors for \"Discover the stories\" and similar\n",
        "                'div[data-ga*=\"Discover\"]', 'div[data-ga*=\"discover\"]',\n",
        "                'div[class*=\"discover\"]', 'div[class*=\"Discover\"]',\n",
        "                'div[data-testid*=\"discover\"]', 'div[id*=\"discover\"]',\n",
        "                'div:has-text(\"Discover the stories\")',\n",
        "                'div:has-text(\"discover the stories\")',\n",
        "                'div:has-text(\"Stay on top\")',\n",
        "                'div:has-text(\"stay on top\")'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors_to_remove:\n",
        "                try:\n",
        "                    await page.evaluate(f\"\"\"selector => {{\n",
        "                        const elements = document.querySelectorAll(selector);\n",
        "                        elements.forEach(el => el.remove());\n",
        "                    }}\"\"\", selector)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Additional text content removal\n",
        "            await page.evaluate(\"\"\"() => {\n",
        "                // Remove elements containing specific text patterns\n",
        "                const unwantedTextPatterns = [\n",
        "                    'Discover the stories of your interest',\n",
        "                    'discover the stories of your interest',\n",
        "                    'Stay on top of technology and startup news',\n",
        "                    'ETPrime stories of the day'\n",
        "                ];\n",
        "\n",
        "                function containsUnwantedText(node) {\n",
        "                    if (node.nodeType === Node.TEXT_NODE) {\n",
        "                        return unwantedTextPatterns.some(pattern =>\n",
        "                            node.textContent.includes(pattern));\n",
        "                    }\n",
        "                    return false;\n",
        "                }\n",
        "\n",
        "                function walkDOM(node) {\n",
        "                    if (node.nodeType === Node.ELEMENT_NODE) {\n",
        "                        // Check if element itself contains unwanted text\n",
        "                        if (unwantedTextPatterns.some(pattern =>\n",
        "                            node.textContent.includes(pattern))) {\n",
        "                            // Check if this is a small element we can remove\n",
        "                            if (node.textContent.length < 500) {\n",
        "                                node.remove();\n",
        "                                return;\n",
        "                            }\n",
        "                        }\n",
        "\n",
        "                        // Check child nodes\n",
        "                        const children = Array.from(node.childNodes);\n",
        "                        for (const child of children) {\n",
        "                            if (containsUnwantedText(child)) {\n",
        "                                // Remove the entire paragraph if it contains the unwanted text\n",
        "                                if (node.tagName === 'P' || node.classList.contains('Normal')) {\n",
        "                                    node.remove();\n",
        "                                    break;\n",
        "                                } else {\n",
        "                                    child.remove();\n",
        "                                }\n",
        "                            } else {\n",
        "                                walkDOM(child);\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                walkDOM(document.body);\n",
        "            }\"\"\")\n",
        "\n",
        "            article_json = None\n",
        "            extraction_attempts = [\n",
        "                f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        if (!reader) return null;\n",
        "                        return {{\n",
        "                            title: reader.title.replace(' - The Economic Times', ''),\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline,\n",
        "                            excerpt: reader.excerpt\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        console.error('Readability error:', e);\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\",\n",
        "\n",
        "                \"\"\"() => {\n",
        "                    const article = document.querySelector('article, .article, .articleContent') ||\n",
        "                                  document.querySelector('.content, .story, .main-content, .Normal');\n",
        "                    if (!article) return null;\n",
        "\n",
        "                    return {\n",
        "                        title: document.title.replace(' - The Economic Times', ''),\n",
        "                        content: article.innerHTML,\n",
        "                        byline: document.querySelector('.byline, .author, .publish-date')?.textContent || '',\n",
        "                        excerpt: document.querySelector('.excerpt, .summary, .synopsis')?.textContent || ''\n",
        "                    };\n",
        "                }\"\"\"\n",
        "            ]\n",
        "\n",
        "            for attempt, extraction_script in enumerate(extraction_attempts, 1):\n",
        "                try:\n",
        "                    print(f\"Attempting extraction method {attempt}...\")\n",
        "                    article_json = await page.evaluate(extraction_script)\n",
        "                    if article_json and article_json.get('content'):\n",
        "                        print(f\"Extracted: {article_json['title'][:50]}...\")\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    print(f\"Extraction attempt {attempt} failed: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if not article_json or not article_json.get('content'):\n",
        "                print(\"All extraction methods failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Clean the content of any remaining promotional text\n",
        "            clean_content = await page.evaluate(\"\"\"(content) => {\n",
        "                const temp = document.createElement('div');\n",
        "                temp.innerHTML = content;\n",
        "\n",
        "                // Remove elements with specific text\n",
        "                const unwantedTexts = [\n",
        "                    'Discover the stories of your interest',\n",
        "                    'Stay on top of technology and startup news',\n",
        "                    'ETPrime stories of the day',\n",
        "                    'BLACK FRIDAY IS LIVE'\n",
        "                ];\n",
        "\n",
        "                unwantedTexts.forEach(text => {\n",
        "                    const elements = Array.from(temp.querySelectorAll('*'));\n",
        "                    elements.forEach(el => {\n",
        "                        if (el.textContent.includes(text)) {\n",
        "                            el.remove();\n",
        "                        }\n",
        "                    });\n",
        "                });\n",
        "\n",
        "                return temp.innerHTML;\n",
        "            }\"\"\", article_json['content'])\n",
        "\n",
        "            article_json['content'] = clean_content\n",
        "\n",
        "            max_pdf_attempts = 3\n",
        "            for attempt in range(max_pdf_attempts):\n",
        "                try:\n",
        "                    print(f\"PDF generation attempt {attempt + 1}...\")\n",
        "                    await page.set_content(f\"\"\"\n",
        "                        <html>\n",
        "                            <head>\n",
        "                                <meta charset=\"UTF-8\">\n",
        "                                <title>{article_json['title']}</title>\n",
        "                                <style>\n",
        "                                    body {{\n",
        "                                        max-width: 800px;\n",
        "                                        margin: 0 auto;\n",
        "                                        padding: 20px;\n",
        "                                        font-family: Arial, sans-serif;\n",
        "                                        line-height: 1.6;\n",
        "                                        color: #333;\n",
        "                                    }}\n",
        "                                    h1 {{\n",
        "                                        font-size: 24px;\n",
        "                                        margin-bottom: 10px;\n",
        "                                        color: #222;\n",
        "                                    }}\n",
        "                                    .byline {{\n",
        "                                        color: #666;\n",
        "                                        margin-bottom: 20px;\n",
        "                                        font-style: italic;\n",
        "                                    }}\n",
        "                                    .excerpt {{\n",
        "                                        font-weight: bold;\n",
        "                                        margin-bottom: 20px;\n",
        "                                        color: #444;\n",
        "                                    }}\n",
        "                                    img {{\n",
        "                                        max-width: 100%;\n",
        "                                        height: auto;\n",
        "                                        margin: 10px 0;\n",
        "                                    }}\n",
        "                                    a {{\n",
        "                                        color: #0066cc;\n",
        "                                        text-decoration: none;\n",
        "                                    }}\n",
        "                                    @media print {{\n",
        "                                        body {{ padding: 0; }}\n",
        "                                    }}\n",
        "                                </style>\n",
        "                            </head>\n",
        "                            <body>\n",
        "                                <h1>{article_json['title']}</h1>\n",
        "                                {f'<div class=\"byline\">{article_json[\"byline\"]}</div>' if article_json.get(\"byline\") else ''}\n",
        "                                {f'<div class=\"excerpt\">{article_json[\"excerpt\"]}</div>' if article_json.get(\"excerpt\") else ''}\n",
        "                                {article_json['content']}\n",
        "                                <div style=\"margin-top: 30px; font-size: 12px; color: #999;\">\n",
        "                                    Source: <a href=\"{url}\">{url}</a>\n",
        "                                </div>\n",
        "                            </body>\n",
        "                        </html>\n",
        "                    \"\"\")\n",
        "\n",
        "                    await page.pdf(\n",
        "                        path=pdf_path,\n",
        "                        format='A4',\n",
        "                        margin={\n",
        "                            'top': '20mm',\n",
        "                            'right': '20mm',\n",
        "                            'bottom': '20mm',\n",
        "                            'left': '20mm'\n",
        "                        },\n",
        "                        print_background=False,\n",
        "                        scale=0.9\n",
        "                    )\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    if attempt == max_pdf_attempts - 1:\n",
        "                        print(f\"PDF generation failed after {max_pdf_attempts} attempts: {str(e)}\")\n",
        "                        await browser.close()\n",
        "                        return False\n",
        "                    print(f\"PDF attempt {attempt + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(2)\n",
        "\n",
        "            await browser.close()\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Step 5: Process multiple URLs with retries\n",
        "async def process_urls_with_retry():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = os.path.join(OUTPUT_DIR, f\"batch_{timestamp}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    failed = []\n",
        "    success_count = 0\n",
        "\n",
        "    for index, url in enumerate(URLS, 1):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing URL {index} of {len(URLS)}\")\n",
        "        print(url)\n",
        "\n",
        "        max_retries = 2\n",
        "        pdf_path = None\n",
        "\n",
        "        for retry in range(max_retries):\n",
        "            try:\n",
        "                async with async_playwright() as p:\n",
        "                    browser = await p.chromium.launch()\n",
        "                    page = await browser.new_page()\n",
        "                    try:\n",
        "                        await page.goto(url, wait_until=\"domcontentloaded\", timeout=30000)\n",
        "                        title = await page.title()\n",
        "                    except:\n",
        "                        title = f\"Article_{index}\"\n",
        "                    await browser.close()\n",
        "\n",
        "                safe_title = sanitize_filename(title[:50])\n",
        "                pdf_name = f\"{index}_{safe_title}.pdf\"\n",
        "                pdf_path = os.path.join(output_dir, pdf_name)\n",
        "\n",
        "                if await create_clean_article_pdf(url, pdf_path):\n",
        "                    print(f\"Successfully created: {pdf_name}\")\n",
        "                    success_count += 1\n",
        "                    if success_count == 1:\n",
        "                        display(HTML(f'<a href=\"{pdf_path}\" download>Download First PDF: {pdf_name}</a>'))\n",
        "                    break\n",
        "                else:\n",
        "                    if retry == max_retries - 1:\n",
        "                        raise Exception(\"PDF creation failed after retries\")\n",
        "                    print(f\"Attempt {retry + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(5)\n",
        "\n",
        "            except Exception as e:\n",
        "                if retry == max_retries - 1:\n",
        "                    print(f\"Failed to process {url}: {str(e)}\")\n",
        "                    failed.append({'url': url, 'error': str(e)})\n",
        "                continue\n",
        "\n",
        "        await asyncio.sleep(3)\n",
        "\n",
        "    if failed:\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerows(failed)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Successfully processed: {success_count}/{len(URLS)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "\n",
        "    if success_count > 0:\n",
        "        display(HTML(f'<a href=\"{output_dir}\" download>Download All PDFs</a>'))\n",
        "\n",
        "    if failed:\n",
        "        print(\"\\nFailed URLs:\")\n",
        "        for item in failed:\n",
        "            print(f\"- {item['url']}\")\n",
        "            print(f\"  Reason: {item['error']}\")\n",
        "\n",
        "# Step 6: Run the process\n",
        "await process_urls_with_retry()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "wHzQg-Sseyhf",
        "outputId": "a00af95e-e3ee-4a65-f1a7-82cb148f3df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "\n",
            "==================================================\n",
            "Processing URL 1 of 1\n",
            "https://economictimes.indiatimes.com/news/economy/policy/nclat-reduces-googles-936-crore-penalty-to-217-crore-over-competition-law-violation/articleshow/119671499.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Trying wait strategy 2...\n",
            "Page loaded successfully\n",
            "Attempting extraction method 1...\n",
            "Extracted: NCLAT reduces Google’s ₹936 crore penalty to ₹217 ...\n",
            "PDF generation attempt 1...\n",
            "Successfully created: 1_NCLAT reduces Googles 936 crore penalty to 217.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250330_084956/1_NCLAT reduces Googles 936 crore penalty to 217.pdf\" download>Download First PDF: 1_NCLAT reduces Googles 936 crore penalty to 217.pdf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing complete!\n",
            "Successfully processed: 1/1\n",
            "Failed: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250330_084956\" download>Download All PDFs</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with date\n"
      ],
      "metadata": {
        "id": "UV3_SuOy7YHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "\n",
        "# List of URLs to process\n",
        "URLS = [\n",
        "    \"https://economictimes.indiatimes.com/news/economy/policy/nclat-reduces-googles-936-crore-penalty-to-217-crore-over-competition-law-violation/articleshow/119671499.cms\"\n",
        "]\n",
        "\n",
        "# Timeout settings\n",
        "NAVIGATION_TIMEOUT = 180000  # 180 seconds\n",
        "REQUEST_TIMEOUT = 45000  # 45 seconds\n",
        "SELECTOR_TIMEOUT = 45000  # 45 seconds\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "async def try_loading_page(page, url):\n",
        "    \"\"\"Multiple strategies to load page content\"\"\"\n",
        "    strategies = [\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.reload(timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"commit\")\n",
        "    ]\n",
        "\n",
        "    for i, strategy in enumerate(strategies, 1):\n",
        "        try:\n",
        "            print(f\"Attempting strategy {i}...\")\n",
        "            await strategy()\n",
        "\n",
        "            wait_strategies = [\n",
        "                lambda: page.wait_for_load_state(\"networkidle\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"article, .article, .articleWrap, .articleContent, .content, .story, .main-content, .Normal\",\n",
        "                                            state=\"attached\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"h1, h2, p\", state=\"attached\", timeout=SELECTOR_TIMEOUT)\n",
        "            ]\n",
        "\n",
        "            for j, wait_strategy in enumerate(wait_strategies, 1):\n",
        "                try:\n",
        "                    print(f\"Trying wait strategy {j}...\")\n",
        "                    await wait_strategy()\n",
        "                    print(\"Page loaded successfully\")\n",
        "                    return True\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Strategy {i} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return False\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(\n",
        "                timeout=NAVIGATION_TIMEOUT,\n",
        "                headless=True,\n",
        "                args=[\n",
        "                    '--disable-gpu',\n",
        "                    '--disable-dev-shm-usage',\n",
        "                    '--disable-setuid-sandbox',\n",
        "                    '--no-sandbox'\n",
        "                ]\n",
        "            )\n",
        "            context = await browser.new_context(\n",
        "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                viewport={'width': 1920, 'height': 1080},\n",
        "                java_script_enabled=True\n",
        "            )\n",
        "            page = await context.new_page()\n",
        "            page.set_default_timeout(NAVIGATION_TIMEOUT)\n",
        "\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=REQUEST_TIMEOUT\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            if not await try_loading_page(page, url):\n",
        "                print(\"All loading strategies failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            selectors_to_remove = [\n",
        "                '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                'header', 'footer', 'iframe', 'script', '.comments',\n",
        "                '.related-news', '.recommendations', '.signup-promo',\n",
        "                '.subscribe', '.hidden', '.modal', '.popup', '.leaderboard',\n",
        "                '.ad-container', '.teaser', '.promo', '.newsletter-signup',\n",
        "                '.social-media', '.sharing', '.recommended', '.trending',\n",
        "                '.most-popular', '.also-read', '.more-from-section',\n",
        "                'div[data-ga*=\"Discover\"]', 'div[data-ga*=\"discover\"]',\n",
        "                'div[class*=\"discover\"]', 'div[class*=\"Discover\"]',\n",
        "                'div[data-testid*=\"discover\"]', 'div[id*=\"discover\"]',\n",
        "                'div:has-text(\"Discover the stories\")',\n",
        "                'div:has-text(\"discover the stories\")',\n",
        "                'div:has-text(\"Stay on top\")',\n",
        "                'div:has-text(\"stay on top\")'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors_to_remove:\n",
        "                try:\n",
        "                    await page.evaluate(f\"\"\"selector => {{\n",
        "                        const elements = document.querySelectorAll(selector);\n",
        "                        elements.forEach(el => el.remove());\n",
        "                    }}\"\"\", selector)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Extract article content with multiple fallback methods\n",
        "            article_data = await page.evaluate(\"\"\"() => {\n",
        "                // Method 1: Try to find article content directly\n",
        "                const articleElement = document.querySelector('article, .article, .articleWrap, .articleContent, .content, .story, .main-content, .Normal');\n",
        "                if (articleElement) {\n",
        "                    return {\n",
        "                        title: document.title.replace(' - The Economic Times', ''),\n",
        "                        content: articleElement.innerHTML,\n",
        "                        date: document.querySelector('.publish_on, .publish-date, .date')?.textContent.trim() || 'Date not available',\n",
        "                        byline: document.querySelector('.byline, .author')?.textContent.trim() || ''\n",
        "                    };\n",
        "                }\n",
        "\n",
        "                // Method 2: Try Readability.js fallback\n",
        "                try {\n",
        "                    const article = new Readability(document.cloneNode(true)).parse();\n",
        "                    if (article) {\n",
        "                        return {\n",
        "                            title: article.title.replace(' - The Economic Times', ''),\n",
        "                            content: article.content,\n",
        "                            date: article.excerpt || 'Date not available',\n",
        "                            byline: article.byline || ''\n",
        "                        };\n",
        "                    }\n",
        "                } catch (e) {}\n",
        "\n",
        "                // Method 3: Fallback to body content\n",
        "                return {\n",
        "                    title: document.title.replace(' - The Economic Times', ''),\n",
        "                    content: document.body.innerHTML,\n",
        "                    date: 'Date not available',\n",
        "                    byline: ''\n",
        "                };\n",
        "            }\"\"\")\n",
        "\n",
        "            if not article_data or not article_data.get('content'):\n",
        "                print(\"Content extraction failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Generate PDF\n",
        "            await page.set_content(f\"\"\"\n",
        "                <html>\n",
        "                    <head>\n",
        "                        <meta charset=\"UTF-8\">\n",
        "                        <title>{article_data['title']}</title>\n",
        "                        <style>\n",
        "                            body {{\n",
        "                                max-width: 800px;\n",
        "                                margin: 0 auto;\n",
        "                                padding: 20px;\n",
        "                                font-family: Arial, sans-serif;\n",
        "                                line-height: 1.6;\n",
        "                                color: #333;\n",
        "                            }}\n",
        "                            h1 {{\n",
        "                                font-size: 24px;\n",
        "                                margin-bottom: 5px;\n",
        "                                color: #222;\n",
        "                            }}\n",
        "                            .article-meta {{\n",
        "                                color: #666;\n",
        "                                margin-bottom: 20px;\n",
        "                                font-size: 14px;\n",
        "                                border-bottom: 1px solid #eee;\n",
        "                                padding-bottom: 15px;\n",
        "                            }}\n",
        "                            .article-meta div {{\n",
        "                                margin-bottom: 5px;\n",
        "                            }}\n",
        "                            .article-content {{\n",
        "                                margin-top: 20px;\n",
        "                            }}\n",
        "                            img {{\n",
        "                                max-width: 100%;\n",
        "                                height: auto;\n",
        "                                margin: 10px 0;\n",
        "                            }}\n",
        "                            @media print {{\n",
        "                                body {{\n",
        "                                    padding: 0;\n",
        "                                    font-size: 12pt;\n",
        "                                }}\n",
        "                                h1 {{\n",
        "                                    font-size: 18pt;\n",
        "                                }}\n",
        "                            }}\n",
        "                        </style>\n",
        "                    </head>\n",
        "                    <body>\n",
        "                        <h1>{article_data['title']}</h1>\n",
        "                        <div class=\"article-meta\">\n",
        "                            <div><strong>Published:</strong> {article_data['date']}</div>\n",
        "                            {f'<div><strong>Author:</strong> {article_data[\"byline\"]}</div>' if article_data[\"byline\"] else ''}\n",
        "                            <div><strong>Source:</strong> <a href=\"{url}\">Economic Times</a></div>\n",
        "                        </div>\n",
        "                        <div class=\"article-content\">\n",
        "                            {article_data['content']}\n",
        "                        </div>\n",
        "                    </body>\n",
        "                </html>\n",
        "            \"\"\")\n",
        "\n",
        "            await page.pdf(\n",
        "                path=pdf_path,\n",
        "                format='A4',\n",
        "                margin={'top': '20mm', 'right': '20mm', 'bottom': '20mm', 'left': '20mm'},\n",
        "                print_background=False,\n",
        "                scale=0.9\n",
        "            )\n",
        "\n",
        "            await browser.close()\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Step 5: Process URLs with retries\n",
        "async def process_urls_with_retry():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = os.path.join(OUTPUT_DIR, f\"batch_{timestamp}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    failed = []\n",
        "    success_count = 0\n",
        "\n",
        "    for index, url in enumerate(URLS, 1):\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing URL {index} of {len(URLS)}\")\n",
        "        print(url)\n",
        "\n",
        "        max_retries = 2\n",
        "        pdf_path = None\n",
        "\n",
        "        for retry in range(max_retries):\n",
        "            try:\n",
        "                pdf_name = f\"article_{index}.pdf\"\n",
        "                pdf_path = os.path.join(output_dir, pdf_name)\n",
        "\n",
        "                if await create_clean_article_pdf(url, pdf_path):\n",
        "                    print(f\"Successfully created: {pdf_name}\")\n",
        "                    success_count += 1\n",
        "                    if success_count == 1:\n",
        "                        display(HTML(f'<a href=\"{pdf_path}\" download>Download First PDF: {pdf_name}</a>'))\n",
        "                    break\n",
        "                else:\n",
        "                    if retry == max_retries - 1:\n",
        "                        raise Exception(\"PDF creation failed after retries\")\n",
        "                    print(f\"Attempt {retry + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(5)\n",
        "\n",
        "            except Exception as e:\n",
        "                if retry == max_retries - 1:\n",
        "                    print(f\"Failed to process {url}: {str(e)}\")\n",
        "                    failed.append({'url': url, 'error': str(e)})\n",
        "                continue\n",
        "\n",
        "        await asyncio.sleep(3)\n",
        "\n",
        "    if failed:\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerows(failed)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Successfully processed: {success_count}/{len(URLS)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "\n",
        "    if success_count > 0:\n",
        "        display(HTML(f'<a href=\"{output_dir}\" download>Download All PDFs</a>'))\n",
        "\n",
        "    if failed:\n",
        "        print(\"\\nFailed URLs:\")\n",
        "        for item in failed:\n",
        "            print(f\"- {item['url']}\")\n",
        "            print(f\"  Reason: {item['error']}\")\n",
        "\n",
        "# Step 6: Run the process\n",
        "await process_urls_with_retry()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "twxDUgVe5X-E",
        "outputId": "c225971d-7416-44bd-88d5-7daabfed40f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "\n",
            "==================================================\n",
            "Processing URL 1 of 1\n",
            "https://economictimes.indiatimes.com/news/economy/policy/nclat-reduces-googles-936-crore-penalty-to-217-crore-over-competition-law-violation/articleshow/119671499.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Trying wait strategy 2...\n",
            "Page loaded successfully\n",
            "Successfully created: article_1.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250330_071746/article_1.pdf\" download>Download First PDF: article_1.pdf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing complete!\n",
            "Successfully processed: 1/1\n",
            "Failed: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250330_071746\" download>Download All PDFs</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "\n",
        "# List of URLs to process\n",
        "URLS = [\n",
        "    \"https://economictimes.indiatimes.com/news/india/next-upi-infosys-co-founder-nandan-nilekani-says-this-new-tech-on-roofs-will-create-millions-of-entrepreneurs/articleshow/119642817.cms\"\n",
        "]\n",
        "\n",
        "# Timeout settings\n",
        "NAVIGATION_TIMEOUT = 180000  # 180 seconds\n",
        "REQUEST_TIMEOUT = 45000  # 45 seconds\n",
        "SELECTOR_TIMEOUT = 45000  # 45 seconds\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "def sanitize_filename(title):\n",
        "    \"\"\"Create a safe filename from article title\"\"\"\n",
        "    keep_chars = (' ', '.', '_', '-')\n",
        "    return \"\".join(c for c in title if c.isalnum() or c in keep_chars).rstrip()\n",
        "\n",
        "async def try_loading_page(page, url):\n",
        "    \"\"\"Multiple strategies to load page content\"\"\"\n",
        "    strategies = [\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.reload(timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"commit\")\n",
        "    ]\n",
        "\n",
        "    for i, strategy in enumerate(strategies, 1):\n",
        "        try:\n",
        "            print(f\"Attempting strategy {i}...\")\n",
        "            await strategy()\n",
        "\n",
        "            wait_strategies = [\n",
        "                lambda: page.wait_for_load_state(\"networkidle\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"article, .article, .articleContent, .content, .story, .main-content, .Normal\",\n",
        "                                            state=\"attached\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"h1, h2, p\", state=\"attached\", timeout=SELECTOR_TIMEOUT)\n",
        "            ]\n",
        "\n",
        "            for j, wait_strategy in enumerate(wait_strategies, 1):\n",
        "                try:\n",
        "                    print(f\"Trying wait strategy {j}...\")\n",
        "                    await wait_strategy()\n",
        "                    print(\"Page loaded successfully\")\n",
        "                    return True\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Strategy {i} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return False\n",
        "\n",
        "async def extract_article_metadata(page):\n",
        "    \"\"\"Optimized metadata extraction focusing on time.jsdtTime element\"\"\"\n",
        "    try:\n",
        "        # Extract title\n",
        "        title = await page.title()\n",
        "        title = title.replace(' - The Economic Times', '').strip()\n",
        "\n",
        "        # Extract CSM number from URL\n",
        "        url = page.url\n",
        "        csm_number = url.split('/')[-1].split('.')[0]\n",
        "\n",
        "        # Fast extraction from time.jsdtTime element\n",
        "        try:\n",
        "            time_element = await page.query_selector('time.jsdtTime')\n",
        "            if time_element:\n",
        "                # First try to get epoch timestamp from data-dt\n",
        "                epoch_ms = await time_element.get_attribute('data-dt')\n",
        "                if epoch_ms:\n",
        "                    dt = datetime.fromtimestamp(int(epoch_ms)/1000)\n",
        "                    return {\n",
        "                        'title': title,\n",
        "                        'csm_number': csm_number,\n",
        "                        'published_date': dt.strftime('%Y%m%d')\n",
        "                    }\n",
        "\n",
        "                # Fallback to text content parsing\n",
        "                date_text = await time_element.text_content()\n",
        "                if date_text and \"Last Updated:\" in date_text:\n",
        "                    date_part = date_text.split(\"Last Updated:\")[1].split(\",\")[0].strip()\n",
        "                    dt = datetime.strptime(date_part, '%b %d %Y')\n",
        "                    return {\n",
        "                        'title': title,\n",
        "                        'csm_number': csm_number,\n",
        "                        'published_date': dt.strftime('%Y%m%d')\n",
        "                    }\n",
        "        except Exception as e:\n",
        "            print(f\"Time element extraction failed: {str(e)}\")\n",
        "\n",
        "        # Fallback to meta tags\n",
        "        try:\n",
        "            date_str = await page.get_attribute('meta[property=\"article:published_time\"]', 'content')\n",
        "            if date_str:\n",
        "                dt = datetime.strptime(date_str.split('T')[0], '%Y-%m-%d')\n",
        "                return {\n",
        "                    'title': title,\n",
        "                    'csm_number': csm_number,\n",
        "                    'published_date': dt.strftime('%Y%m%d')\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Final fallback to current date\n",
        "        return {\n",
        "            'title': title,\n",
        "            'csm_number': csm_number,\n",
        "            'published_date': datetime.now().strftime('%Y%m%d')\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Metadata extraction error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(\n",
        "                timeout=NAVIGATION_TIMEOUT,\n",
        "                headless=True,\n",
        "                args=[\n",
        "                    '--disable-gpu',\n",
        "                    '--disable-dev-shm-usage',\n",
        "                    '--disable-setuid-sandbox',\n",
        "                    '--no-sandbox'\n",
        "                ]\n",
        "            )\n",
        "            context = await browser.new_context(\n",
        "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                viewport={'width': 1920, 'height': 1080},\n",
        "                java_script_enabled=True\n",
        "            )\n",
        "            page = await context.new_page()\n",
        "            page.set_default_timeout(NAVIGATION_TIMEOUT)\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=REQUEST_TIMEOUT\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            if not await try_loading_page(page, url):\n",
        "                print(\"All loading strategies failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Extract metadata first (optimized for time.jsdtTime)\n",
        "            metadata = await extract_article_metadata(page)\n",
        "            if not metadata:\n",
        "                print(\"Failed to extract article metadata\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            selectors_to_remove = [\n",
        "                '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                'header', 'footer', 'iframe', 'script', '.comments',\n",
        "                '.related-news', '.recommendations', '.signup-promo',\n",
        "                '.subscribe', '.hidden', '.modal', '.popup', '.leaderboard',\n",
        "                '.ad-container', '.teaser', '.promo', '.newsletter-signup',\n",
        "                '.social-media', '.sharing', '.recommended', '.trending',\n",
        "                '.most-popular', '.also-read', '.more-from-section',\n",
        "                'div[data-ga*=\"Discover\"]', 'div[data-ga*=\"discover\"]',\n",
        "                'div[class*=\"discover\"]', 'div[class*=\"Discover\"]',\n",
        "                'div[data-testid*=\"discover\"]', 'div[id*=\"discover\"]',\n",
        "                'div:has-text(\"Discover the stories\")',\n",
        "                'div:has-text(\"discover the stories\")',\n",
        "                'div:has-text(\"Stay on top\")',\n",
        "                'div:has-text(\"stay on top\")'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors_to_remove:\n",
        "                try:\n",
        "                    await page.evaluate(f\"\"\"selector => {{\n",
        "                        const elements = document.querySelectorAll(selector);\n",
        "                        elements.forEach(el => el.remove());\n",
        "                    }}\"\"\", selector)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Additional text content removal\n",
        "            await page.evaluate(\"\"\"() => {\n",
        "                const unwantedTextPatterns = [\n",
        "                    'Discover the stories of your interest',\n",
        "                    'discover the stories of your interest',\n",
        "                    'Stay on top of technology and startup news',\n",
        "                    'ETPrime stories of the day'\n",
        "                ];\n",
        "\n",
        "                function walkDOM(node) {\n",
        "                    if (node.nodeType === Node.ELEMENT_NODE) {\n",
        "                        if (unwantedTextPatterns.some(pattern =>\n",
        "                            node.textContent.includes(pattern))) {\n",
        "                            if (node.textContent.length < 500) {\n",
        "                                node.remove();\n",
        "                                return;\n",
        "                            }\n",
        "                        }\n",
        "                        Array.from(node.childNodes).forEach(walkDOM);\n",
        "                    }\n",
        "                }\n",
        "                walkDOM(document.body);\n",
        "            }\"\"\")\n",
        "\n",
        "            # Extract article content\n",
        "            article_json = None\n",
        "            extraction_attempts = [\n",
        "                f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        if (!reader) return null;\n",
        "                        return {{\n",
        "                            title: reader.title,\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline,\n",
        "                            excerpt: reader.excerpt\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        console.error('Readability error:', e);\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\",\n",
        "                \"\"\"() => {\n",
        "                    const article = document.querySelector('article, .article, .articleContent') ||\n",
        "                                  document.querySelector('.content, .story, .main-content, .Normal');\n",
        "                    if (!article) return null;\n",
        "                    return {\n",
        "                        title: document.title,\n",
        "                        content: article.innerHTML,\n",
        "                        byline: document.querySelector('.byline, .author, .publish-date')?.textContent || '',\n",
        "                        excerpt: document.querySelector('.excerpt, .summary, .synopsis')?.textContent || ''\n",
        "                    };\n",
        "                }\"\"\"\n",
        "            ]\n",
        "\n",
        "            for attempt, extraction_script in enumerate(extraction_attempts, 1):\n",
        "                try:\n",
        "                    article_json = await page.evaluate(extraction_script)\n",
        "                    if article_json and article_json.get('content'):\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if not article_json or not article_json.get('content'):\n",
        "                print(\"All extraction methods failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Generate PDF\n",
        "            try:\n",
        "                await page.set_content(f\"\"\"\n",
        "                    <html>\n",
        "                        <head>\n",
        "                            <meta charset=\"UTF-8\">\n",
        "                            <title>{article_json['title']}</title>\n",
        "                            <style>\n",
        "                                body {{\n",
        "                                    max-width: 800px;\n",
        "                                    margin: 0 auto;\n",
        "                                    padding: 20px;\n",
        "                                    font-family: Arial, sans-serif;\n",
        "                                    line-height: 1.6;\n",
        "                                    color: #333;\n",
        "                                }}\n",
        "                                h1 {{\n",
        "                                    font-size: 24px;\n",
        "                                    margin-bottom: 10px;\n",
        "                                    color: #222;\n",
        "                                }}\n",
        "                                .byline {{\n",
        "                                    color: #666;\n",
        "                                    margin-bottom: 20px;\n",
        "                                    font-style: italic;\n",
        "                                }}\n",
        "                                .excerpt {{\n",
        "                                    font-weight: bold;\n",
        "                                    margin-bottom: 20px;\n",
        "                                    color: #444;\n",
        "                                }}\n",
        "                                img {{\n",
        "                                    max-width: 100%;\n",
        "                                    height: auto;\n",
        "                                    margin: 10px 0;\n",
        "                                }}\n",
        "                                a {{\n",
        "                                    color: #0066cc;\n",
        "                                    text-decoration: none;\n",
        "                                }}\n",
        "                                @media print {{\n",
        "                                    body {{ padding: 0; }}\n",
        "                                }}\n",
        "                            </style>\n",
        "                        </head>\n",
        "                        <body>\n",
        "                            <h1>{article_json['title']}</h1>\n",
        "                            {f'<div class=\"byline\">{article_json[\"byline\"]}</div>' if article_json.get(\"byline\") else ''}\n",
        "                            {f'<div class=\"excerpt\">{article_json[\"excerpt\"]}</div>' if article_json.get(\"excerpt\") else ''}\n",
        "                            {article_json['content']}\n",
        "                            <div style=\"margin-top: 30px; font-size: 12px; color: #999;\">\n",
        "                                Source: <a href=\"{url}\">{url}</a>\n",
        "                            </div>\n",
        "                        </body>\n",
        "                    </html>\n",
        "                \"\"\")\n",
        "\n",
        "                await page.pdf(\n",
        "                    path=pdf_path,\n",
        "                    format='A4',\n",
        "                    margin={\n",
        "                        'top': '20mm',\n",
        "                        'right': '20mm',\n",
        "                        'bottom': '20mm',\n",
        "                        'left': '20mm'\n",
        "                    },\n",
        "                    print_background=False,\n",
        "                    scale=0.9\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"PDF generation failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            await browser.close()\n",
        "            return metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "async def process_urls_with_retry():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    batch_dir = os.path.join(OUTPUT_DIR, f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "    os.makedirs(batch_dir, exist_ok=True)\n",
        "\n",
        "    failed = []\n",
        "    success_count = 0\n",
        "\n",
        "    for url in URLS:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing URL: {url}\")\n",
        "\n",
        "        for retry in range(2):  # Max 2 retries\n",
        "            try:\n",
        "                # Create temp path first\n",
        "                temp_pdf = os.path.join(batch_dir, \"temp.pdf\")\n",
        "\n",
        "                # Process article and get metadata\n",
        "                result = await create_clean_article_pdf(url, temp_pdf)\n",
        "                if not result:\n",
        "                    raise Exception(\"PDF creation failed\")\n",
        "\n",
        "                # Generate final filename\n",
        "                safe_title = sanitize_filename(result['title'][:50])  # Limit title length\n",
        "                pdf_name = f\"{result['published_date']}_{result['csm_number']}_{safe_title}.pdf\"\n",
        "                final_path = os.path.join(batch_dir, pdf_name)\n",
        "\n",
        "                # Rename temp file to final filename\n",
        "                os.rename(temp_pdf, final_path)\n",
        "\n",
        "                print(f\"Successfully created: {pdf_name}\")\n",
        "                success_count += 1\n",
        "                if success_count == 1:\n",
        "                    display(HTML(f'<a href=\"{final_path}\" download>Download First PDF: {pdf_name}</a>'))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if retry == 1:  # Last attempt failed\n",
        "                    print(f\"Failed to process {url}: {str(e)}\")\n",
        "                    failed.append({'url': url, 'error': str(e)})\n",
        "                else:\n",
        "                    print(f\"Attempt {retry + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(3)\n",
        "                continue\n",
        "\n",
        "    # Save failed URLs if any\n",
        "    if failed:\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerows(failed)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Successfully processed: {success_count}/{len(URLS)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "\n",
        "    if success_count > 0:\n",
        "        display(HTML(f'<a href=\"{batch_dir}\" download>Download All PDFs</a>'))\n",
        "\n",
        "    if failed:\n",
        "        print(\"\\nFailed URLs:\")\n",
        "        for item in failed:\n",
        "            print(f\"- {item['url']}\")\n",
        "            print(f\"  Reason: {item['error']}\")\n",
        "\n",
        "# Step 6: Run the process\n",
        "await process_urls_with_retry()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "AQXWlU8tO4Vm",
        "outputId": "a1f7122e-bc6e-4f55-f790-bc3008f01a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Future exception was never retrieved\n",
            "future: <Future finished exception=TargetClosedError('Target page, context or browser has been closed\\nCall log:\\n  - waiting for locator(\"meta[property=\\\\\"article:published_time\\\\\"]\")\\n')>\n",
            "playwright._impl._errors.TargetClosedError: Target page, context or browser has been closed\n",
            "Call log:\n",
            "  - waiting for locator(\"meta[property=\\\"article:published_time\\\"]\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "\n",
            "==================================================\n",
            "Processing URL: https://economictimes.indiatimes.com/news/india/next-upi-infosys-co-founder-nandan-nilekani-says-this-new-tech-on-roofs-will-create-millions-of-entrepreneurs/articleshow/119642817.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Trying wait strategy 2...\n",
            "Page loaded successfully\n",
            "Successfully created: 20250328_119642817_Next UPI Infosys co-founder Nandan Nilekani say.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250330_091305/20250328_119642817_Next UPI Infosys co-founder Nandan Nilekani say.pdf\" download>Download First PDF: 20250328_119642817_Next UPI Infosys co-founder Nandan Nilekani say.pdf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing complete!\n",
            "Successfully processed: 1/1\n",
            "Failed: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250330_091305\" download>Download All PDFs</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BEST OF BEST\n"
      ],
      "metadata": {
        "id": "F1LGaoidW1hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "\n",
        "# List of URLs to process\n",
        "URLS = [\n",
        "    \"https://economictimes.indiatimes.com/news/india/next-upi-infosys-co-founder-nandan-nilekani-says-this-new-tech-on-roofs-will-create-millions-of-entrepreneurs/articleshow/119642817.cms\"\n",
        "]\n",
        "\n",
        "# Timeout settings\n",
        "NAVIGATION_TIMEOUT = 180000  # 180 seconds\n",
        "REQUEST_TIMEOUT = 45000  # 45 seconds\n",
        "SELECTOR_TIMEOUT = 45000  # 45 seconds\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "def sanitize_filename(title):\n",
        "    \"\"\"Create a safe filename from article title\"\"\"\n",
        "    keep_chars = (' ', '.', '_', '-')\n",
        "    return \"\".join(c for c in title if c.isalnum() or c in keep_chars).rstrip()\n",
        "\n",
        "async def try_loading_page(page, url):\n",
        "    \"\"\"Multiple strategies to load page content\"\"\"\n",
        "    strategies = [\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.reload(timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"commit\")\n",
        "    ]\n",
        "\n",
        "    for i, strategy in enumerate(strategies, 1):\n",
        "        try:\n",
        "            print(f\"Attempting strategy {i}...\")\n",
        "            await strategy()\n",
        "\n",
        "            wait_strategies = [\n",
        "                lambda: page.wait_for_load_state(\"networkidle\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"article, .article, .articleContent, .content, .story, .main-content, .Normal\",\n",
        "                                            state=\"attached\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"h1, h2, p\", state=\"attached\", timeout=SELECTOR_TIMEOUT)\n",
        "            ]\n",
        "\n",
        "            for j, wait_strategy in enumerate(wait_strategies, 1):\n",
        "                try:\n",
        "                    print(f\"Trying wait strategy {j}...\")\n",
        "                    await wait_strategy()\n",
        "                    print(\"Page loaded successfully\")\n",
        "                    return True\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Strategy {i} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return False\n",
        "\n",
        "async def extract_article_metadata(page):\n",
        "    \"\"\"Ultra-fast metadata extraction with direct DOM access\"\"\"\n",
        "    try:\n",
        "        # Get title and CSM number first (fast operations)\n",
        "        title = (await page.title()).replace(' - The Economic Times', '').strip()\n",
        "        csm_number = page.url.split('/')[-1].split('.')[0]\n",
        "\n",
        "        # 1. FIRST TRY: Directly access the time element's data-dt attribute (fastest)\n",
        "        try:\n",
        "            epoch_ms = await page.evaluate('''() => {\n",
        "                const el = document.querySelector('time.jsdtTime');\n",
        "                return el ? el.getAttribute('data-dt') : null;\n",
        "            }''')\n",
        "            if epoch_ms:\n",
        "                dt = datetime.fromtimestamp(int(epoch_ms)/1000)\n",
        "                return {\n",
        "                    'title': title,\n",
        "                    'csm_number': csm_number,\n",
        "                    'published_date': dt.strftime('%Y%m%d')\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 2. SECOND TRY: Direct text extraction from time element (fast)\n",
        "        try:\n",
        "            date_text = await page.evaluate('''() => {\n",
        "                const el = document.querySelector('time.jsdtTime');\n",
        "                return el ? el.textContent : null;\n",
        "            }''')\n",
        "\n",
        "            if date_text and \"Last Updated:\" in date_text:\n",
        "                # Extract just \"Mar 28, 2025\" part\n",
        "                date_part = date_text.split(\"Last Updated:\")[1].split(\",\")[0].strip()\n",
        "                dt = datetime.strptime(date_part, '%b %d %Y')\n",
        "                return {\n",
        "                    'title': title,\n",
        "                    'csm_number': csm_number,\n",
        "                    'published_date': dt.strftime('%Y%m%d')\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 3. THIRD TRY: Check for common meta tags (still relatively fast)\n",
        "        try:\n",
        "            meta_date = await page.evaluate('''() => {\n",
        "                const el = document.querySelector('meta[property=\"article:published_time\"]');\n",
        "                return el ? el.content : null;\n",
        "            }''')\n",
        "            if meta_date:\n",
        "                dt = datetime.strptime(meta_date.split('T')[0], '%Y-%m-%d')\n",
        "                return {\n",
        "                    'title': title,\n",
        "                    'csm_number': csm_number,\n",
        "                    'published_date': dt.strftime('%Y%m%d')\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Final fallback\n",
        "        return {\n",
        "            'title': title,\n",
        "            'csm_number': csm_number,\n",
        "            'published_date': datetime.now().strftime('%Y%m%d')\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Metadata error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(\n",
        "                timeout=NAVIGATION_TIMEOUT,\n",
        "                headless=True,\n",
        "                args=[\n",
        "                    '--disable-gpu',\n",
        "                    '--disable-dev-shm-usage',\n",
        "                    '--disable-setuid-sandbox',\n",
        "                    '--no-sandbox'\n",
        "                ]\n",
        "            )\n",
        "            context = await browser.new_context(\n",
        "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                viewport={'width': 1920, 'height': 1080},\n",
        "                java_script_enabled=True\n",
        "            )\n",
        "            page = await context.new_page()\n",
        "            page.set_default_timeout(NAVIGATION_TIMEOUT)\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=REQUEST_TIMEOUT\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            if not await try_loading_page(page, url):\n",
        "                print(\"All loading strategies failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Extract metadata first (optimized for time.jsdtTime)\n",
        "            metadata = await extract_article_metadata(page)\n",
        "            if not metadata:\n",
        "                print(\"Failed to extract article metadata\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            selectors_to_remove = [\n",
        "                '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                'header', 'footer', 'iframe', 'script', '.comments',\n",
        "                '.related-news', '.recommendations', '.signup-promo',\n",
        "                '.subscribe', '.hidden', '.modal', '.popup', '.leaderboard',\n",
        "                '.ad-container', '.teaser', '.promo', '.newsletter-signup',\n",
        "                '.social-media', '.sharing', '.recommended', '.trending',\n",
        "                '.most-popular', '.also-read', '.more-from-section',\n",
        "                'div[data-ga*=\"Discover\"]', 'div[data-ga*=\"discover\"]',\n",
        "                'div[class*=\"discover\"]', 'div[class*=\"Discover\"]',\n",
        "                'div[data-testid*=\"discover\"]', 'div[id*=\"discover\"]',\n",
        "                'div:has-text(\"Discover the stories\")',\n",
        "                'div:has-text(\"discover the stories\")',\n",
        "                'div:has-text(\"Stay on top\")',\n",
        "                'div:has-text(\"stay on top\")'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors_to_remove:\n",
        "                try:\n",
        "                    await page.evaluate(f\"\"\"selector => {{\n",
        "                        const elements = document.querySelectorAll(selector);\n",
        "                        elements.forEach(el => el.remove());\n",
        "                    }}\"\"\", selector)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Additional text content removal\n",
        "            await page.evaluate(\"\"\"() => {\n",
        "                const unwantedTextPatterns = [\n",
        "                    'Discover the stories of your interest',\n",
        "                    'discover the stories of your interest',\n",
        "                    'Stay on top of technology and startup news',\n",
        "                    'ETPrime stories of the day'\n",
        "                ];\n",
        "\n",
        "                function walkDOM(node) {\n",
        "                    if (node.nodeType === Node.ELEMENT_NODE) {\n",
        "                        if (unwantedTextPatterns.some(pattern =>\n",
        "                            node.textContent.includes(pattern))) {\n",
        "                            if (node.textContent.length < 500) {\n",
        "                                node.remove();\n",
        "                                return;\n",
        "                            }\n",
        "                        }\n",
        "                        Array.from(node.childNodes).forEach(walkDOM);\n",
        "                    }\n",
        "                }\n",
        "                walkDOM(document.body);\n",
        "            }\"\"\")\n",
        "\n",
        "            # Extract article content\n",
        "            article_json = None\n",
        "            extraction_attempts = [\n",
        "                f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        if (!reader) return null;\n",
        "                        return {{\n",
        "                            title: reader.title,\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline,\n",
        "                            excerpt: reader.excerpt\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        console.error('Readability error:', e);\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\",\n",
        "                \"\"\"() => {\n",
        "                    const article = document.querySelector('article, .article, .articleContent') ||\n",
        "                                  document.querySelector('.content, .story, .main-content, .Normal');\n",
        "                    if (!article) return null;\n",
        "                    return {\n",
        "                        title: document.title,\n",
        "                        content: article.innerHTML,\n",
        "                        byline: document.querySelector('.byline, .author, .publish-date')?.textContent || '',\n",
        "                        excerpt: document.querySelector('.excerpt, .summary, .synopsis')?.textContent || ''\n",
        "                    };\n",
        "                }\"\"\"\n",
        "            ]\n",
        "\n",
        "            for attempt, extraction_script in enumerate(extraction_attempts, 1):\n",
        "                try:\n",
        "                    article_json = await page.evaluate(extraction_script)\n",
        "                    if article_json and article_json.get('content'):\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if not article_json or not article_json.get('content'):\n",
        "                print(\"All extraction methods failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Generate PDF\n",
        "            try:\n",
        "                await page.set_content(f\"\"\"\n",
        "                    <html>\n",
        "                        <head>\n",
        "                            <meta charset=\"UTF-8\">\n",
        "                            <title>{article_json['title']}</title>\n",
        "                            <style>\n",
        "                                body {{\n",
        "                                    max-width: 800px;\n",
        "                                    margin: 0 auto;\n",
        "                                    padding: 20px;\n",
        "                                    font-family: Arial, sans-serif;\n",
        "                                    line-height: 1.6;\n",
        "                                    color: #333;\n",
        "                                }}\n",
        "                                h1 {{\n",
        "                                    font-size: 24px;\n",
        "                                    margin-bottom: 10px;\n",
        "                                    color: #222;\n",
        "                                }}\n",
        "                                .byline {{\n",
        "                                    color: #666;\n",
        "                                    margin-bottom: 20px;\n",
        "                                    font-style: italic;\n",
        "                                }}\n",
        "                                .excerpt {{\n",
        "                                    font-weight: bold;\n",
        "                                    margin-bottom: 20px;\n",
        "                                    color: #444;\n",
        "                                }}\n",
        "                                img {{\n",
        "                                    max-width: 100%;\n",
        "                                    height: auto;\n",
        "                                    margin: 10px 0;\n",
        "                                }}\n",
        "                                a {{\n",
        "                                    color: #0066cc;\n",
        "                                    text-decoration: none;\n",
        "                                }}\n",
        "                                @media print {{\n",
        "                                    body {{ padding: 0; }}\n",
        "                                }}\n",
        "                            </style>\n",
        "                        </head>\n",
        "                        <body>\n",
        "                            <h1>{article_json['title']}</h1>\n",
        "                            {f'<div class=\"byline\">{article_json[\"byline\"]}</div>' if article_json.get(\"byline\") else ''}\n",
        "                            {f'<div class=\"excerpt\">{article_json[\"excerpt\"]}</div>' if article_json.get(\"excerpt\") else ''}\n",
        "                            {article_json['content']}\n",
        "                            <div style=\"margin-top: 30px; font-size: 12px; color: #999;\">\n",
        "                                Source: <a href=\"{url}\">{url}</a>\n",
        "                            </div>\n",
        "                        </body>\n",
        "                    </html>\n",
        "                \"\"\")\n",
        "\n",
        "                await page.pdf(\n",
        "                    path=pdf_path,\n",
        "                    format='A4',\n",
        "                    margin={\n",
        "                        'top': '20mm',\n",
        "                        'right': '20mm',\n",
        "                        'bottom': '20mm',\n",
        "                        'left': '20mm'\n",
        "                    },\n",
        "                    print_background=False,\n",
        "                    scale=0.9\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"PDF generation failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            await browser.close()\n",
        "            return metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "async def process_urls_with_retry():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    batch_dir = os.path.join(OUTPUT_DIR, f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "    os.makedirs(batch_dir, exist_ok=True)\n",
        "\n",
        "    failed = []\n",
        "    success_count = 0\n",
        "\n",
        "    for url in URLS:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing URL: {url}\")\n",
        "\n",
        "        for retry in range(2):  # Max 2 retries\n",
        "            try:\n",
        "                # Create temp path first\n",
        "                temp_pdf = os.path.join(batch_dir, \"temp.pdf\")\n",
        "\n",
        "                # Process article and get metadata\n",
        "                result = await create_clean_article_pdf(url, temp_pdf)\n",
        "                if not result:\n",
        "                    raise Exception(\"PDF creation failed\")\n",
        "\n",
        "                # Generate final filename\n",
        "                safe_title = sanitize_filename(result['title'][:50])  # Limit title length\n",
        "                pdf_name = f\"{result['published_date']}_{result['csm_number']}_{safe_title}.pdf\"\n",
        "                final_path = os.path.join(batch_dir, pdf_name)\n",
        "\n",
        "                # Rename temp file to final filename\n",
        "                os.rename(temp_pdf, final_path)\n",
        "\n",
        "                print(f\"Successfully created: {pdf_name}\")\n",
        "                success_count += 1\n",
        "                if success_count == 1:\n",
        "                    display(HTML(f'<a href=\"{final_path}\" download>Download First PDF: {pdf_name}</a>'))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if retry == 1:  # Last attempt failed\n",
        "                    print(f\"Failed to process {url}: {str(e)}\")\n",
        "                    failed.append({'url': url, 'error': str(e)})\n",
        "                else:\n",
        "                    print(f\"Attempt {retry + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(3)\n",
        "                continue\n",
        "\n",
        "    # Save failed URLs if any\n",
        "    if failed:\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerows(failed)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Successfully processed: {success_count}/{len(URLS)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "\n",
        "    if success_count > 0:\n",
        "        display(HTML(f'<a href=\"{batch_dir}\" download>Download All PDFs</a>'))\n",
        "\n",
        "    if failed:\n",
        "        print(\"\\nFailed URLs:\")\n",
        "        for item in failed:\n",
        "            print(f\"- {item['url']}\")\n",
        "            print(f\"  Reason: {item['error']}\")\n",
        "\n",
        "# Step 6: Run the process\n",
        "await process_urls_with_retry()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "hfNhDpB7UYki",
        "outputId": "163e8d8f-a354-4074-d24c-db0af332e5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "\n",
            "==================================================\n",
            "Processing URL: https://economictimes.indiatimes.com/news/india/next-upi-infosys-co-founder-nandan-nilekani-says-this-new-tech-on-roofs-will-create-millions-of-entrepreneurs/articleshow/119642817.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Trying wait strategy 2...\n",
            "Page loaded successfully\n",
            "Successfully created: 20250328_119642817_Next UPI Infosys co-founder Nandan Nilekani say.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250330_091746/20250328_119642817_Next UPI Infosys co-founder Nandan Nilekani say.pdf\" download>Download First PDF: 20250328_119642817_Next UPI Infosys co-founder Nandan Nilekani say.pdf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing complete!\n",
            "Successfully processed: 1/1\n",
            "Failed: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250330_091746\" download>Download All PDFs</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install playwright requests nest-asyncio\n",
        "!playwright install\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import os\n",
        "import csv\n",
        "from urllib.parse import urljoin\n",
        "from playwright.async_api import async_playwright\n",
        "import requests\n",
        "from IPython.display import display, HTML\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "\n",
        "# Apply nest_asyncio to make async work in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 3: Configuration\n",
        "OUTPUT_DIR = \"/content/articles\"\n",
        "FAILED_CSV = \"/content/failed_urls.csv\"\n",
        "BASE_URL = \"https://economictimes.indiatimes.com\"\n",
        "\n",
        "# List of URLs to process\n",
        "URLS = [\n",
        "\n",
        "]\n",
        "\n",
        "# Timeout settings\n",
        "NAVIGATION_TIMEOUT = 180000  # 180 seconds\n",
        "REQUEST_TIMEOUT = 45000  # 45 seconds\n",
        "SELECTOR_TIMEOUT = 45000  # 45 seconds\n",
        "\n",
        "# Step 4: Define functions\n",
        "def validate_url(url):\n",
        "    \"\"\"Convert relative URLs to absolute and validate\"\"\"\n",
        "    if url.startswith('/'):\n",
        "        return urljoin(BASE_URL, url)\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        raise ValueError(f\"Invalid URL format: {url}\")\n",
        "    return url\n",
        "\n",
        "def sanitize_filename(title):\n",
        "    \"\"\"Create a safe filename from article title\"\"\"\n",
        "    keep_chars = (' ', '.', '_', '-')\n",
        "    return \"\".join(c for c in title if c.isalnum() or c in keep_chars).rstrip()\n",
        "\n",
        "async def try_loading_page(page, url):\n",
        "    \"\"\"Multiple strategies to load page content\"\"\"\n",
        "    strategies = [\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.reload(timeout=NAVIGATION_TIMEOUT, wait_until=\"domcontentloaded\"),\n",
        "        lambda: page.goto(url, timeout=NAVIGATION_TIMEOUT, wait_until=\"commit\")\n",
        "    ]\n",
        "\n",
        "    for i, strategy in enumerate(strategies, 1):\n",
        "        try:\n",
        "            print(f\"Attempting strategy {i}...\")\n",
        "            await strategy()\n",
        "\n",
        "            wait_strategies = [\n",
        "                lambda: page.wait_for_load_state(\"networkidle\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"article, .article, .articleContent, .content, .story, .main-content, .Normal\",\n",
        "                                            state=\"attached\", timeout=SELECTOR_TIMEOUT),\n",
        "                lambda: page.wait_for_selector(\"h1, h2, p\", state=\"attached\", timeout=SELECTOR_TIMEOUT)\n",
        "            ]\n",
        "\n",
        "            for j, wait_strategy in enumerate(wait_strategies, 1):\n",
        "                try:\n",
        "                    print(f\"Trying wait strategy {j}...\")\n",
        "                    await wait_strategy()\n",
        "                    print(\"Page loaded successfully\")\n",
        "                    return True\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Strategy {i} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return False\n",
        "\n",
        "async def extract_article_metadata(page):\n",
        "    \"\"\"Ultra-fast metadata extraction with direct DOM access\"\"\"\n",
        "    try:\n",
        "        # Get title and CSM number first (fast operations)\n",
        "        title = (await page.title()).replace(' - The Economic Times', '').strip()\n",
        "        csm_number = page.url.split('/')[-1].split('.')[0]\n",
        "\n",
        "        # 1. FIRST TRY: Directly access the time element's data-dt attribute (fastest)\n",
        "        try:\n",
        "            epoch_ms = await page.evaluate('''() => {\n",
        "                const el = document.querySelector('time.jsdtTime');\n",
        "                return el ? el.getAttribute('data-dt') : null;\n",
        "            }''')\n",
        "            if epoch_ms:\n",
        "                dt = datetime.fromtimestamp(int(epoch_ms)/1000)\n",
        "                return {\n",
        "                    'title': title,\n",
        "                    'csm_number': csm_number,\n",
        "                    'published_date': dt.strftime('%Y%m%d'),\n",
        "                    'display_date': dt.strftime('%d %B %Y')  # Added display format\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 2. SECOND TRY: Direct text extraction from time element (fast)\n",
        "        try:\n",
        "            date_text = await page.evaluate('''() => {\n",
        "                const el = document.querySelector('time.jsdtTime');\n",
        "                return el ? el.textContent : null;\n",
        "            }''')\n",
        "\n",
        "            if date_text and \"Last Updated:\" in date_text:\n",
        "                # Extract just \"Mar 28, 2025\" part\n",
        "                date_part = date_text.split(\"Last Updated:\")[1].split(\",\")[0].strip()\n",
        "                dt = datetime.strptime(date_part, '%b %d %Y')\n",
        "                return {\n",
        "                    'title': title,\n",
        "                    'csm_number': csm_number,\n",
        "                    'published_date': dt.strftime('%Y%m%d'),\n",
        "                    'display_date': dt.strftime('%d %B %Y')  # Added display format\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 3. THIRD TRY: Check for common meta tags (still relatively fast)\n",
        "        try:\n",
        "            meta_date = await page.evaluate('''() => {\n",
        "                const el = document.querySelector('meta[property=\"article:published_time\"]');\n",
        "                return el ? el.content : null;\n",
        "            }''')\n",
        "            if meta_date:\n",
        "                dt = datetime.strptime(meta_date.split('T')[0], '%Y-%m-%d')\n",
        "                return {\n",
        "                    'title': title,\n",
        "                    'csm_number': csm_number,\n",
        "                    'published_date': dt.strftime('%Y%m%d'),\n",
        "                    'display_date': dt.strftime('%d %B %Y')  # Added display format\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Final fallback\n",
        "        current_date = datetime.now()\n",
        "        return {\n",
        "            'title': title,\n",
        "            'csm_number': csm_number,\n",
        "            'published_date': current_date.strftime('%Y%m%d'),\n",
        "            'display_date': current_date.strftime('%d %B %Y')  # Added display format\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Metadata error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "async def create_clean_article_pdf(url, pdf_path):\n",
        "    try:\n",
        "        url = validate_url(url)\n",
        "        async with async_playwright() as p:\n",
        "            browser = await p.chromium.launch(\n",
        "                timeout=NAVIGATION_TIMEOUT,\n",
        "                headless=True,\n",
        "                args=[\n",
        "                    '--disable-gpu',\n",
        "                    '--disable-dev-shm-usage',\n",
        "                    '--disable-setuid-sandbox',\n",
        "                    '--no-sandbox'\n",
        "                ]\n",
        "            )\n",
        "            context = await browser.new_context(\n",
        "                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                viewport={'width': 1920, 'height': 1080},\n",
        "                java_script_enabled=True\n",
        "            )\n",
        "            page = await context.new_page()\n",
        "            page.set_default_timeout(NAVIGATION_TIMEOUT)\n",
        "\n",
        "            # Load Readability.js\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    \"https://raw.githubusercontent.com/mozilla/readability/master/Readability.js\",\n",
        "                    timeout=REQUEST_TIMEOUT\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "                readability_script = response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Readability.js load failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            if not await try_loading_page(page, url):\n",
        "                print(\"All loading strategies failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Extract metadata first (optimized for time.jsdtTime)\n",
        "            metadata = await extract_article_metadata(page)\n",
        "            if not metadata:\n",
        "                print(\"Failed to extract article metadata\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            selectors_to_remove = [\n",
        "                '.recommendedStories', '.socialShares', '.newsletter',\n",
        "                '.discoverTheStory', '.et_related', '[class*=\"ad\"]',\n",
        "                'header', 'footer', 'iframe', 'script', '.comments',\n",
        "                '.related-news', '.recommendations', '.signup-promo',\n",
        "                '.subscribe', '.hidden', '.modal', '.popup', '.leaderboard',\n",
        "                '.ad-container', '.teaser', '.promo', '.newsletter-signup',\n",
        "                '.social-media', '.sharing', '.recommended', '.trending',\n",
        "                '.most-popular', '.also-read', '.more-from-section',\n",
        "                'div[data-ga*=\"Discover\"]', 'div[data-ga*=\"discover\"]',\n",
        "                'div[class*=\"discover\"]', 'div[class*=\"Discover\"]',\n",
        "                'div[data-testid*=\"discover\"]', 'div[id*=\"discover\"]',\n",
        "                'div:has-text(\"Discover the stories\")',\n",
        "                'div:has-text(\"discover the stories\")',\n",
        "                'div:has-text(\"Stay on top\")',\n",
        "                'div:has-text(\"stay on top\")'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors_to_remove:\n",
        "                try:\n",
        "                    await page.evaluate(f\"\"\"selector => {{\n",
        "                        const elements = document.querySelectorAll(selector);\n",
        "                        elements.forEach(el => el.remove());\n",
        "                    }}\"\"\", selector)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Additional text content removal\n",
        "            await page.evaluate(\"\"\"() => {\n",
        "                const unwantedTextPatterns = [\n",
        "                    'Discover the stories of your interest',\n",
        "                    'discover the stories of your interest',\n",
        "                    'Stay on top of technology and startup news',\n",
        "                    'ETPrime stories of the day'\n",
        "                ];\n",
        "\n",
        "                function walkDOM(node) {\n",
        "                    if (node.nodeType === Node.ELEMENT_NODE) {\n",
        "                        if (unwantedTextPatterns.some(pattern =>\n",
        "                            node.textContent.includes(pattern))) {\n",
        "                            if (node.textContent.length < 500) {\n",
        "                                node.remove();\n",
        "                                return;\n",
        "                            }\n",
        "                        }\n",
        "                        Array.from(node.childNodes).forEach(walkDOM);\n",
        "                    }\n",
        "                }\n",
        "                walkDOM(document.body);\n",
        "            }\"\"\")\n",
        "\n",
        "            # Extract article content\n",
        "            article_json = None\n",
        "            extraction_attempts = [\n",
        "                f\"\"\"() => {{\n",
        "                    {readability_script}\n",
        "                    try {{\n",
        "                        const reader = new Readability(document.cloneNode(true)).parse();\n",
        "                        if (!reader) return null;\n",
        "                        return {{\n",
        "                            title: reader.title,\n",
        "                            content: reader.content,\n",
        "                            byline: reader.byline,\n",
        "                            excerpt: reader.excerpt\n",
        "                        }};\n",
        "                    }} catch (e) {{\n",
        "                        console.error('Readability error:', e);\n",
        "                        return null;\n",
        "                    }}\n",
        "                }}\"\"\",\n",
        "                \"\"\"() => {\n",
        "                    const article = document.querySelector('article, .article, .articleContent') ||\n",
        "                                  document.querySelector('.content, .story, .main-content, .Normal');\n",
        "                    if (!article) return null;\n",
        "                    return {\n",
        "                        title: document.title,\n",
        "                        content: article.innerHTML,\n",
        "                        byline: document.querySelector('.byline, .author, .publish-date')?.textContent || '',\n",
        "                        excerpt: document.querySelector('.excerpt, .summary, .synopsis')?.textContent || ''\n",
        "                    };\n",
        "                }\"\"\"\n",
        "            ]\n",
        "\n",
        "            for attempt, extraction_script in enumerate(extraction_attempts, 1):\n",
        "                try:\n",
        "                    article_json = await page.evaluate(extraction_script)\n",
        "                    if article_json and article_json.get('content'):\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if not article_json or not article_json.get('content'):\n",
        "                print(\"All extraction methods failed\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            # Generate PDF with date under heading\n",
        "            try:\n",
        "                await page.set_content(f\"\"\"\n",
        "                    <html>\n",
        "                        <head>\n",
        "                            <meta charset=\"UTF-8\">\n",
        "                            <title>{article_json['title']}</title>\n",
        "                            <style>\n",
        "                                body {{\n",
        "                                    max-width: 800px;\n",
        "                                    margin: 0 auto;\n",
        "                                    padding: 20px;\n",
        "                                    font-family: Arial, sans-serif;\n",
        "                                    line-height: 1.6;\n",
        "                                    color: #333;\n",
        "                                }}\n",
        "                                h1 {{\n",
        "                                    font-size: 24px;\n",
        "                                    margin-bottom: 5px;\n",
        "                                    color: #222;\n",
        "                                }}\n",
        "                                .article-date {{\n",
        "                                    color: #666;\n",
        "                                    margin-bottom: 15px;\n",
        "                                    font-size: 14px;\n",
        "                                }}\n",
        "                                .byline {{\n",
        "                                    color: #666;\n",
        "                                    margin-bottom: 20px;\n",
        "                                    font-style: italic;\n",
        "                                }}\n",
        "                                .excerpt {{\n",
        "                                    font-weight: bold;\n",
        "                                    margin-bottom: 20px;\n",
        "                                    color: #444;\n",
        "                                }}\n",
        "                                img {{\n",
        "                                    max-width: 100%;\n",
        "                                    height: auto;\n",
        "                                    margin: 10px 0;\n",
        "                                }}\n",
        "                                a {{\n",
        "                                    color: #0066cc;\n",
        "                                    text-decoration: none;\n",
        "                                }}\n",
        "                                @media print {{\n",
        "                                    body {{ padding: 0; }}\n",
        "                                }}\n",
        "                            </style>\n",
        "                        </head>\n",
        "                        <body>\n",
        "                            <h1>{article_json['title']}</h1>\n",
        "                            <div class=\"article-date\">{metadata['display_date']}</div>\n",
        "                            {f'<div class=\"byline\">{article_json[\"byline\"]}</div>' if article_json.get(\"byline\") else ''}\n",
        "                            {f'<div class=\"excerpt\">{article_json[\"excerpt\"]}</div>' if article_json.get(\"excerpt\") else ''}\n",
        "                            {article_json['content']}\n",
        "                            <div style=\"margin-top: 30px; font-size: 12px; color: #999;\">\n",
        "                                Source: <a href=\"{url}\">{url}</a>\n",
        "                            </div>\n",
        "                        </body>\n",
        "                    </html>\n",
        "                \"\"\")\n",
        "\n",
        "                await page.pdf(\n",
        "                    path=pdf_path,\n",
        "                    format='A4',\n",
        "                    margin={\n",
        "                        'top': '20mm',\n",
        "                        'right': '20mm',\n",
        "                        'bottom': '20mm',\n",
        "                        'left': '20mm'\n",
        "                    },\n",
        "                    print_background=False,\n",
        "                    scale=0.9\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"PDF generation failed: {str(e)}\")\n",
        "                await browser.close()\n",
        "                return False\n",
        "\n",
        "            await browser.close()\n",
        "            return metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "async def process_urls_with_retry():\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    batch_dir = os.path.join(OUTPUT_DIR, f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
        "    os.makedirs(batch_dir, exist_ok=True)\n",
        "\n",
        "    failed = []\n",
        "    success_count = 0\n",
        "\n",
        "    for url in URLS:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing URL: {url}\")\n",
        "\n",
        "        for retry in range(2):  # Max 2 retries\n",
        "            try:\n",
        "                # Create temp path first\n",
        "                temp_pdf = os.path.join(batch_dir, \"temp.pdf\")\n",
        "\n",
        "                # Process article and get metadata\n",
        "                result = await create_clean_article_pdf(url, temp_pdf)\n",
        "                if not result:\n",
        "                    raise Exception(\"PDF creation failed\")\n",
        "\n",
        "                # Generate final filename\n",
        "                safe_title = sanitize_filename(result['title'][:50])\n",
        "                pdf_name = f\"{result['published_date']}_{result['csm_number']}_{safe_title}.pdf\"\n",
        "                final_path = os.path.join(batch_dir, pdf_name)\n",
        "\n",
        "                # Rename temp file to final filename\n",
        "                os.rename(temp_pdf, final_path)\n",
        "\n",
        "                print(f\"Successfully created: {pdf_name}\")\n",
        "                success_count += 1\n",
        "                if success_count == 1:\n",
        "                    display(HTML(f'<a href=\"{final_path}\" download>Download First PDF: {pdf_name}</a>'))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if retry == 1:  # Last attempt failed\n",
        "                    print(f\"Failed to process {url}: {str(e)}\")\n",
        "                    failed.append({'url': url, 'error': str(e)})\n",
        "                else:\n",
        "                    print(f\"Attempt {retry + 1} failed, retrying...\")\n",
        "                    await asyncio.sleep(3)\n",
        "                continue\n",
        "\n",
        "    # Save failed URLs if any\n",
        "    if failed:\n",
        "        with open(FAILED_CSV, 'w') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=['url', 'error'])\n",
        "            writer.writeheader()\n",
        "            writer.writerows(failed)\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(f\"Successfully processed: {success_count}/{len(URLS)}\")\n",
        "    print(f\"Failed: {len(failed)}\")\n",
        "\n",
        "    if success_count > 0:\n",
        "        display(HTML(f'<a href=\"{batch_dir}\" download>Download All PDFs</a>'))\n",
        "\n",
        "    if failed:\n",
        "        print(\"\\nFailed URLs:\")\n",
        "        for item in failed:\n",
        "            print(f\"- {item['url']}\")\n",
        "            print(f\"  Reason: {item['error']}\")\n",
        "\n",
        "# Step 6: Run the process\n",
        "await process_urls_with_retry()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "C-zwEAa7XNgK",
        "outputId": "ac828c54-adf8-45a7-f855-459a39c39fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:859:52)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:957:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:946:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:122:7)\n",
            "\n",
            "==================================================\n",
            "Processing URL: https://economictimes.indiatimes.com/news/india/next-upi-infosys-co-founder-nandan-nilekani-says-this-new-tech-on-roofs-will-create-millions-of-entrepreneurs/articleshow/119642817.cms\n",
            "Attempting strategy 1...\n",
            "Trying wait strategy 1...\n",
            "Trying wait strategy 2...\n",
            "Page loaded successfully\n",
            "Successfully created: 20250328_119642817_Next UPI Infosys co-founder Nandan Nilekani say.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250330_092555/20250328_119642817_Next UPI Infosys co-founder Nandan Nilekani say.pdf\" download>Download First PDF: 20250328_119642817_Next UPI Infosys co-founder Nandan Nilekani say.pdf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing complete!\n",
            "Successfully processed: 1/1\n",
            "Failed: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"/content/articles/batch_20250330_092555\" download>Download All PDFs</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W43Yl2R0fH-o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}